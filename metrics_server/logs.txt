* 
* ==> Audit <==
* |--------------|------------------------------------------------------|----------|------|---------|-----------------------|-----------------------|
|   Command    |                         Args                         | Profile  | User | Version |      Start Time       |       End Time        |
|--------------|------------------------------------------------------|----------|------|---------|-----------------------|-----------------------|
| docker-env   |                                                      | minikube | hadi | v1.30.0 | 02 Dec 24 09:21 +0430 | 02 Dec 24 09:21 +0430 |
| ssh          |                                                      | minikube | hadi | v1.30.0 | 02 Dec 24 10:00 +0430 |                       |
| ssh          |                                                      | minikube | hadi | v1.30.0 | 02 Dec 24 10:03 +0430 |                       |
| ssh          |                                                      | minikube | hadi | v1.30.0 | 02 Dec 24 10:07 +0430 |                       |
| start        |                                                      | minikube | hadi | v1.30.0 | 02 Dec 24 10:28 +0430 | 02 Dec 24 10:28 +0430 |
| docker-env   | minikube docker-env                                  | minikube | hadi | v1.30.0 | 02 Dec 24 10:36 +0430 | 02 Dec 24 10:36 +0430 |
| docker-env   | minikube docker-env                                  | minikube | hadi | v1.30.0 | 02 Dec 24 10:55 +0430 | 02 Dec 24 10:55 +0430 |
| start        |                                                      | minikube | hadi | v1.30.0 | 02 Dec 24 11:16 +0430 | 02 Dec 24 11:16 +0430 |
| docker-env   | minikube docker-env                                  | minikube | hadi | v1.30.0 | 02 Dec 24 11:19 +0430 | 02 Dec 24 11:19 +0430 |
| start        |                                                      | minikube | hadi | v1.30.0 | 02 Dec 24 11:25 +0430 | 02 Dec 24 11:26 +0430 |
| stop         |                                                      | minikube | hadi | v1.30.0 | 02 Dec 24 11:28 +0430 | 02 Dec 24 11:28 +0430 |
| start        |                                                      | minikube | hadi | v1.30.0 | 02 Dec 24 11:28 +0430 | 02 Dec 24 11:28 +0430 |
| start        |                                                      | minikube | hadi | v1.30.0 | 02 Dec 24 12:13 +0430 | 02 Dec 24 12:14 +0430 |
| ssh          |                                                      | minikube | hadi | v1.30.0 | 02 Dec 24 12:40 +0430 |                       |
| addons       | enable dashboard                                     | minikube | hadi | v1.30.0 | 02 Dec 24 12:49 +0430 | 02 Dec 24 12:49 +0430 |
| dashboard    |                                                      | minikube | hadi | v1.30.0 | 02 Dec 24 12:49 +0430 |                       |
| ip           |                                                      | minikube | hadi | v1.30.0 | 02 Dec 24 12:52 +0430 | 02 Dec 24 12:52 +0430 |
| image        | load                                                 | minikube | hadi | v1.30.0 | 02 Dec 24 13:55 +0430 |                       |
|              | hadirezaie/springkubernetes1:v2                      |          |      |         |                       |                       |
| docker-env   |                                                      | minikube | hadi | v1.30.0 | 02 Dec 24 13:57 +0430 | 02 Dec 24 13:57 +0430 |
| addons       | enable registry                                      | minikube | hadi | v1.30.0 | 02 Dec 24 14:06 +0430 |                       |
| start        |                                                      | minikube | hadi | v1.30.0 | 02 Dec 24 14:07 +0430 | 02 Dec 24 14:08 +0430 |
| docker-env   |                                                      | minikube | hadi | v1.30.0 | 02 Dec 24 14:09 +0430 | 02 Dec 24 14:09 +0430 |
| start        |                                                      | minikube | hadi | v1.30.0 | 02 Dec 24 15:05 +0430 | 02 Dec 24 15:05 +0430 |
| docker-env   |                                                      | minikube | hadi | v1.30.0 | 02 Dec 24 15:06 +0430 | 02 Dec 24 15:06 +0430 |
| start        |                                                      | minikube | hadi | v1.30.0 | 02 Dec 24 21:32 +0430 | 02 Dec 24 21:32 +0430 |
| stop         |                                                      | minikube | hadi | v1.30.0 | 02 Dec 24 22:07 +0430 | 02 Dec 24 22:07 +0430 |
| delete       |                                                      | minikube | hadi | v1.30.0 | 02 Dec 24 22:07 +0430 | 02 Dec 24 22:07 +0430 |
| start        |                                                      | minikube | hadi | v1.30.0 | 02 Dec 24 22:08 +0430 | 02 Dec 24 22:08 +0430 |
| docker-env   |                                                      | minikube | hadi | v1.30.0 | 02 Dec 24 22:17 +0430 | 02 Dec 24 22:17 +0430 |
| docker-env   |                                                      | minikube | hadi | v1.30.0 | 02 Dec 24 22:19 +0430 | 02 Dec 24 22:19 +0430 |
| addons       | enable dashboard                                     | minikube | hadi | v1.30.0 | 03 Dec 24 07:16 +0430 | 03 Dec 24 07:16 +0430 |
| dashboard    |                                                      | minikube | hadi | v1.30.0 | 03 Dec 24 07:16 +0430 |                       |
| addons       | enable dashboard                                     | minikube | hadi | v1.30.0 | 03 Dec 24 08:58 +0430 | 03 Dec 24 08:58 +0430 |
| dashboard    |                                                      | minikube | hadi | v1.30.0 | 03 Dec 24 08:58 +0430 |                       |
| service      | kubernetes-springboot-deployment                     | minikube | hadi | v1.30.0 | 03 Dec 24 09:00 +0430 | 03 Dec 24 09:00 +0430 |
|              | -n kubernetes-springboot                             |          |      |         |                       |                       |
| ip           |                                                      | minikube | hadi | v1.30.0 | 03 Dec 24 09:13 +0430 | 03 Dec 24 09:13 +0430 |
| update-check |                                                      | minikube | hadi | v1.30.0 | 03 Dec 24 09:32 +0430 | 03 Dec 24 09:32 +0430 |
| start        | --kubernetes-version=v1.27.0                         | minikube | hadi | v1.30.0 | 03 Dec 24 09:32 +0430 | 03 Dec 24 09:44 +0430 |
|              | --driver=docker                                      |          |      |         |                       |                       |
| cache        | add                                                  | minikube | hadi | v1.30.0 | 03 Dec 24 09:36 +0430 |                       |
|              | registry.k8s.io/metrics-server/metrics-server:v0.7.2 |          |      |         |                       |                       |
| image        | load                                                 | minikube | hadi | v1.30.0 | 03 Dec 24 09:37 +0430 |                       |
|              | registry.k8s.io/metrics-server/metrics-server:v0.7.2 |          |      |         |                       |                       |
| image        | load                                                 | minikube | hadi | v1.30.0 | 03 Dec 24 09:39 +0430 |                       |
|              | registry.k8s.io/metrics-server/metrics-server:v0.7.2 |          |      |         |                       |                       |
| stop         |                                                      | minikube | hadi | v1.30.0 | 03 Dec 24 09:39 +0430 | 03 Dec 24 09:39 +0430 |
| start        |                                                      | minikube | hadi | v1.30.0 | 03 Dec 24 09:39 +0430 | 03 Dec 24 09:44 +0430 |
| start        | --kubernetes-version=v1.27.0                         | minikube | hadi | v1.30.0 | 03 Dec 24 09:45 +0430 | 03 Dec 24 09:48 +0430 |
|              | --driver=docker                                      |          |      |         |                       |                       |
| cache        | add                                                  | minikube | hadi | v1.30.0 | 03 Dec 24 09:47 +0430 |                       |
|              | registry.k8s.io/metrics-server/metrics-server:v0.7.2 |          |      |         |                       |                       |
| stop         |                                                      | minikube | hadi | v1.30.0 | 03 Dec 24 09:51 +0430 | 03 Dec 24 09:51 +0430 |
| start        |                                                      | minikube | hadi | v1.30.0 | 03 Dec 24 09:51 +0430 |                       |
| config       | set kubernetes-version                               | minikube | hadi | v1.30.0 | 03 Dec 24 09:53 +0430 | 03 Dec 24 09:53 +0430 |
|              | v1.27.0-rc.0                                         |          |      |         |                       |                       |
| start        |                                                      | minikube | hadi | v1.30.0 | 03 Dec 24 09:54 +0430 |                       |
| delete       |                                                      | minikube | hadi | v1.30.0 | 03 Dec 24 09:55 +0430 | 03 Dec 24 09:55 +0430 |
| start        | --kubernetes-version=v1.27.0-rc.0                    | minikube | hadi | v1.30.0 | 03 Dec 24 09:55 +0430 | 03 Dec 24 09:56 +0430 |
| docker-env   |                                                      | minikube | hadi | v1.30.0 | 03 Dec 24 10:06 +0430 | 03 Dec 24 10:06 +0430 |
| start        |                                                      | minikube | hadi | v1.30.0 | 03 Dec 24 10:09 +0430 | 03 Dec 24 10:09 +0430 |
| docker-env   | minikube docker-env                                  | minikube | hadi | v1.30.0 | 03 Dec 24 10:09 +0430 | 03 Dec 24 10:09 +0430 |
| stop         |                                                      | minikube | hadi | v1.30.0 | 03 Dec 24 10:15 +0430 | 03 Dec 24 10:15 +0430 |
| start        |                                                      | minikube | hadi | v1.30.0 | 03 Dec 24 10:15 +0430 | 03 Dec 24 10:15 +0430 |
| addons       | enable dashboard                                     | minikube | hadi | v1.30.0 | 03 Dec 24 10:34 +0430 | 03 Dec 24 10:34 +0430 |
| dashboard    |                                                      | minikube | hadi | v1.30.0 | 03 Dec 24 10:34 +0430 |                       |
| ip           |                                                      | minikube | hadi | v1.30.0 | 03 Dec 24 10:36 +0430 | 03 Dec 24 10:36 +0430 |
| image        | load                                                 | minikube | hadi | v1.30.0 | 03 Dec 24 11:57 +0430 |                       |
|              | k8s.gcr.io/metrics-server/metrics-server:v0.5.0      |          |      |         |                       |                       |
|--------------|------------------------------------------------------|----------|------|---------|-----------------------|-----------------------|

* 
* ==> Last Start <==
* Log file created at: 2024/12/03 10:15:25
Running on machine: hadi-Latitude-5530
Binary: Built with gc go1.20.2 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1203 10:15:25.999363 1021519 out.go:296] Setting OutFile to fd 1 ...
I1203 10:15:25.999471 1021519 out.go:348] isatty.IsTerminal(1) = true
I1203 10:15:25.999475 1021519 out.go:309] Setting ErrFile to fd 2...
I1203 10:15:25.999479 1021519 out.go:348] isatty.IsTerminal(2) = true
I1203 10:15:25.999550 1021519 root.go:336] Updating PATH: /home/hadi/.minikube/bin
I1203 10:15:25.999560 1021519 oci.go:567] shell is pointing to dockerd inside minikube. will unset to use host
I1203 10:15:25.999918 1021519 out.go:303] Setting JSON to false
I1203 10:15:26.001358 1021519 start.go:125] hostinfo: {"hostname":"hadi-Latitude-5530","uptime":63090,"bootTime":1733141636,"procs":392,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"24.10","kernelVersion":"6.11.0-9-generic","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"49b82532-9440-4b78-acdc-2a039e5dfd53"}
I1203 10:15:26.001419 1021519 start.go:135] virtualization: kvm host
I1203 10:15:26.005790 1021519 out.go:177] 😄  minikube v1.30.0 on Ubuntu 24.10
I1203 10:15:26.013224 1021519 out.go:177]     ▪ MINIKUBE_ACTIVE_DOCKERD=minikube
I1203 10:15:26.013133 1021519 notify.go:220] Checking for updates...
I1203 10:15:26.021517 1021519 out.go:177]     ▪ MINIKUBE_EXISTING_DOCKER_HOST=unix:///var/run/docker.sock
I1203 10:15:26.025535 1021519 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.0-rc.0
I1203 10:15:26.025568 1021519 driver.go:365] Setting default libvirt URI to qemu:///system
I1203 10:15:26.043393 1021519 docker.go:121] docker version: linux-27.3.1:Docker Engine - Community
I1203 10:15:26.043451 1021519 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1203 10:15:26.100289 1021519 info.go:266] docker info: {ID:b3b72d47-9b31-4b56-adc2-02cd98efcc44 Containers:3 ContainersRunning:0 ContainersPaused:0 ContainersStopped:3 Images:4 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:23 OomKillDisable:false NGoroutines:40 SystemTime:2024-12-03 10:15:26.092799653 +0430 +0430 LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.11.0-9-generic OperatingSystem:Ubuntu 24.10 OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:15901360128 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:hadi-Latitude-5530 Labels:[] ExperimentalBuild:false ServerVersion:27.3.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:57f17b0a6295a39009d861b89e3b3b87b005ca27 Expected:57f17b0a6295a39009d861b89e3b3b87b005ca27} RuncCommit:{ID:v1.1.14-0-g2c9f560 Expected:v1.1.14-0-g2c9f560} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShadowedPaths:[/usr/libexec/docker/cli-plugins/docker-buildx] ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.17.1-desktop.1] map[Name:compose Path:/usr/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShadowedPaths:[/usr/libexec/docker/cli-plugins/docker-compose] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.29.7-desktop.1] map[Name:debug Path:/usr/lib/docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.37] map[Name:desktop Path:/usr/lib/docker/cli-plugins/docker-desktop SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Alpha) Vendor:Docker Inc. Version:v0.0.15] map[Name:dev Path:/usr/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/usr/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:feedback Path:/usr/lib/docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:/usr/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.3.0] map[Name:sbom Path:/usr/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/usr/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.14.0]] Warnings:<nil>}}
I1203 10:15:26.100380 1021519 docker.go:294] overlay module found
I1203 10:15:26.104333 1021519 out.go:177] ✨  Using the docker driver based on existing profile
I1203 10:15:26.108804 1021519 start.go:295] selected driver: docker
I1203 10:15:26.108811 1021519 start.go:859] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase-builds:v0.0.38-1680381266-16207@sha256:426ee3dccdda8a0d40cd86fbdbe440858176d8d4d9c37319b1c702ef226aea93 Memory:3700 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.0-rc.0 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.0-rc.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/hadi:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP:}
I1203 10:15:26.108893 1021519 start.go:870] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1203 10:15:26.108978 1021519 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1203 10:15:26.163438 1021519 info.go:266] docker info: {ID:b3b72d47-9b31-4b56-adc2-02cd98efcc44 Containers:3 ContainersRunning:0 ContainersPaused:0 ContainersStopped:3 Images:4 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:23 OomKillDisable:false NGoroutines:40 SystemTime:2024-12-03 10:15:26.156814268 +0430 +0430 LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.11.0-9-generic OperatingSystem:Ubuntu 24.10 OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:15901360128 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:hadi-Latitude-5530 Labels:[] ExperimentalBuild:false ServerVersion:27.3.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:57f17b0a6295a39009d861b89e3b3b87b005ca27 Expected:57f17b0a6295a39009d861b89e3b3b87b005ca27} RuncCommit:{ID:v1.1.14-0-g2c9f560 Expected:v1.1.14-0-g2c9f560} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShadowedPaths:[/usr/libexec/docker/cli-plugins/docker-buildx] ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.17.1-desktop.1] map[Name:compose Path:/usr/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShadowedPaths:[/usr/libexec/docker/cli-plugins/docker-compose] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.29.7-desktop.1] map[Name:debug Path:/usr/lib/docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.37] map[Name:desktop Path:/usr/lib/docker/cli-plugins/docker-desktop SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Alpha) Vendor:Docker Inc. Version:v0.0.15] map[Name:dev Path:/usr/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/usr/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:feedback Path:/usr/lib/docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:/usr/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.3.0] map[Name:sbom Path:/usr/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/usr/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.14.0]] Warnings:<nil>}}
I1203 10:15:26.164205 1021519 cni.go:84] Creating CNI manager for ""
I1203 10:15:26.164213 1021519 cni.go:157] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1203 10:15:26.164218 1021519 start_flags.go:319] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase-builds:v0.0.38-1680381266-16207@sha256:426ee3dccdda8a0d40cd86fbdbe440858176d8d4d9c37319b1c702ef226aea93 Memory:3700 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.0-rc.0 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.0-rc.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/hadi:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP:}
I1203 10:15:26.168074 1021519 out.go:177] 👍  Starting control plane node minikube in cluster minikube
I1203 10:15:26.171687 1021519 cache.go:120] Beginning downloading kic base image for docker with docker
I1203 10:15:26.175258 1021519 out.go:177] 🚜  Pulling base image ...
I1203 10:15:26.182273 1021519 preload.go:132] Checking if preload exists for k8s version v1.27.0-rc.0 and runtime docker
I1203 10:15:26.182327 1021519 preload.go:148] Found local preload: /home/hadi/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.0-rc.0-docker-overlay2-amd64.tar.lz4
I1203 10:15:26.182335 1021519 cache.go:57] Caching tarball of preloaded images
I1203 10:15:26.182357 1021519 image.go:79] Checking for gcr.io/k8s-minikube/kicbase-builds:v0.0.38-1680381266-16207@sha256:426ee3dccdda8a0d40cd86fbdbe440858176d8d4d9c37319b1c702ef226aea93 in local docker daemon
I1203 10:15:26.182434 1021519 preload.go:174] Found /home/hadi/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.0-rc.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1203 10:15:26.182441 1021519 cache.go:60] Finished verifying existence of preloaded tar for  v1.27.0-rc.0 on docker
I1203 10:15:26.182555 1021519 profile.go:148] Saving config to /home/hadi/.minikube/profiles/minikube/config.json ...
I1203 10:15:26.196337 1021519 image.go:83] Found gcr.io/k8s-minikube/kicbase-builds:v0.0.38-1680381266-16207@sha256:426ee3dccdda8a0d40cd86fbdbe440858176d8d4d9c37319b1c702ef226aea93 in local docker daemon, skipping pull
I1203 10:15:26.196350 1021519 cache.go:143] gcr.io/k8s-minikube/kicbase-builds:v0.0.38-1680381266-16207@sha256:426ee3dccdda8a0d40cd86fbdbe440858176d8d4d9c37319b1c702ef226aea93 exists in daemon, skipping load
I1203 10:15:26.196365 1021519 cache.go:193] Successfully downloaded all kic artifacts
I1203 10:15:26.196389 1021519 start.go:364] acquiring machines lock for minikube: {Name:mk6843f0d5c7f0086e6af6bdb4065460cf0dcc9f Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1203 10:15:26.196448 1021519 start.go:368] acquired machines lock for "minikube" in 39.225µs
I1203 10:15:26.196462 1021519 start.go:96] Skipping create...Using existing machine configuration
I1203 10:15:26.196465 1021519 fix.go:55] fixHost starting: 
I1203 10:15:26.200854 1021519 out.go:177] 📌  Noticed you have an activated docker-env on docker driver in this terminal:
W1203 10:15:26.204677 1021519 out.go:239] ❗  Please re-eval your docker-env, To ensure your environment variables have updated ports:

	'minikube -p minikube docker-env'

	
I1203 10:15:26.204754 1021519 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1203 10:15:26.219698 1021519 fix.go:103] recreateIfNeeded on minikube: state=Stopped err=<nil>
W1203 10:15:26.219710 1021519 fix.go:129] unexpected machine state, will restart: <nil>
I1203 10:15:26.223528 1021519 out.go:177] 🔄  Restarting existing docker container for "minikube" ...
I1203 10:15:26.227178 1021519 cli_runner.go:164] Run: docker start minikube
I1203 10:15:26.481213 1021519 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1203 10:15:26.490767 1021519 kic.go:426] container "minikube" state is running.
I1203 10:15:26.491039 1021519 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1203 10:15:26.502201 1021519 profile.go:148] Saving config to /home/hadi/.minikube/profiles/minikube/config.json ...
I1203 10:15:26.502398 1021519 machine.go:88] provisioning docker machine ...
I1203 10:15:26.502409 1021519 ubuntu.go:169] provisioning hostname "minikube"
I1203 10:15:26.502445 1021519 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1203 10:15:26.515395 1021519 main.go:141] libmachine: Using SSH client type: native
I1203 10:15:26.515754 1021519 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80e3e0] 0x811480 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1203 10:15:26.515762 1021519 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1203 10:15:26.516308 1021519 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:39092->127.0.0.1:32768: read: connection reset by peer
I1203 10:15:29.518749 1021519 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:39096->127.0.0.1:32768: read: connection reset by peer
I1203 10:15:32.642068 1021519 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1203 10:15:32.642142 1021519 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1203 10:15:32.658669 1021519 main.go:141] libmachine: Using SSH client type: native
I1203 10:15:32.659053 1021519 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80e3e0] 0x811480 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1203 10:15:32.659071 1021519 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1203 10:15:32.762819 1021519 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1203 10:15:32.762833 1021519 ubuntu.go:175] set auth options {CertDir:/home/hadi/.minikube CaCertPath:/home/hadi/.minikube/certs/ca.pem CaPrivateKeyPath:/home/hadi/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/hadi/.minikube/machines/server.pem ServerKeyPath:/home/hadi/.minikube/machines/server-key.pem ClientKeyPath:/home/hadi/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/hadi/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/hadi/.minikube}
I1203 10:15:32.762863 1021519 ubuntu.go:177] setting up certificates
I1203 10:15:32.762875 1021519 provision.go:83] configureAuth start
I1203 10:15:32.762938 1021519 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1203 10:15:32.779734 1021519 provision.go:138] copyHostCerts
I1203 10:15:32.779763 1021519 exec_runner.go:144] found /home/hadi/.minikube/ca.pem, removing ...
I1203 10:15:32.779766 1021519 exec_runner.go:207] rm: /home/hadi/.minikube/ca.pem
I1203 10:15:32.779810 1021519 exec_runner.go:151] cp: /home/hadi/.minikube/certs/ca.pem --> /home/hadi/.minikube/ca.pem (1070 bytes)
I1203 10:15:32.779867 1021519 exec_runner.go:144] found /home/hadi/.minikube/cert.pem, removing ...
I1203 10:15:32.779869 1021519 exec_runner.go:207] rm: /home/hadi/.minikube/cert.pem
I1203 10:15:32.779888 1021519 exec_runner.go:151] cp: /home/hadi/.minikube/certs/cert.pem --> /home/hadi/.minikube/cert.pem (1115 bytes)
I1203 10:15:32.779918 1021519 exec_runner.go:144] found /home/hadi/.minikube/key.pem, removing ...
I1203 10:15:32.779919 1021519 exec_runner.go:207] rm: /home/hadi/.minikube/key.pem
I1203 10:15:32.779929 1021519 exec_runner.go:151] cp: /home/hadi/.minikube/certs/key.pem --> /home/hadi/.minikube/key.pem (1679 bytes)
I1203 10:15:32.779954 1021519 provision.go:112] generating server cert: /home/hadi/.minikube/machines/server.pem ca-key=/home/hadi/.minikube/certs/ca.pem private-key=/home/hadi/.minikube/certs/ca-key.pem org=hadi.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I1203 10:15:32.847358 1021519 provision.go:172] copyRemoteCerts
I1203 10:15:32.847391 1021519 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1203 10:15:32.847414 1021519 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1203 10:15:32.858002 1021519 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/hadi/.minikube/machines/minikube/id_rsa Username:docker}
I1203 10:15:32.937120 1021519 ssh_runner.go:362] scp /home/hadi/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1070 bytes)
I1203 10:15:32.952507 1021519 ssh_runner.go:362] scp /home/hadi/.minikube/machines/server.pem --> /etc/docker/server.pem (1196 bytes)
I1203 10:15:32.966654 1021519 ssh_runner.go:362] scp /home/hadi/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I1203 10:15:32.983897 1021519 provision.go:86] duration metric: configureAuth took 220.988968ms
I1203 10:15:32.983912 1021519 ubuntu.go:193] setting minikube options for container-runtime
I1203 10:15:32.984076 1021519 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.0-rc.0
I1203 10:15:32.984124 1021519 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1203 10:15:32.999868 1021519 main.go:141] libmachine: Using SSH client type: native
I1203 10:15:33.000425 1021519 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80e3e0] 0x811480 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1203 10:15:33.000437 1021519 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1203 10:15:33.120612 1021519 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1203 10:15:33.120631 1021519 ubuntu.go:71] root file system type: overlay
I1203 10:15:33.120782 1021519 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I1203 10:15:33.121441 1021519 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1203 10:15:33.136592 1021519 main.go:141] libmachine: Using SSH client type: native
I1203 10:15:33.136817 1021519 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80e3e0] 0x811480 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1203 10:15:33.136858 1021519 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1203 10:15:33.269962 1021519 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1203 10:15:33.270036 1021519 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1203 10:15:33.289122 1021519 main.go:141] libmachine: Using SSH client type: native
I1203 10:15:33.289703 1021519 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80e3e0] 0x811480 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1203 10:15:33.289725 1021519 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1203 10:15:33.411979 1021519 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1203 10:15:33.412007 1021519 machine.go:91] provisioned docker machine in 6.909599383s
I1203 10:15:33.412020 1021519 start.go:300] post-start starting for "minikube" (driver="docker")
I1203 10:15:33.412027 1021519 start.go:328] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1203 10:15:33.412104 1021519 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1203 10:15:33.412137 1021519 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1203 10:15:33.438990 1021519 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/hadi/.minikube/machines/minikube/id_rsa Username:docker}
I1203 10:15:33.519770 1021519 ssh_runner.go:195] Run: cat /etc/os-release
I1203 10:15:33.522819 1021519 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1203 10:15:33.522830 1021519 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1203 10:15:33.522837 1021519 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1203 10:15:33.522840 1021519 info.go:137] Remote host: Ubuntu 20.04.5 LTS
I1203 10:15:33.522847 1021519 filesync.go:126] Scanning /home/hadi/.minikube/addons for local assets ...
I1203 10:15:33.522904 1021519 filesync.go:126] Scanning /home/hadi/.minikube/files for local assets ...
I1203 10:15:33.522921 1021519 start.go:303] post-start completed in 110.894839ms
I1203 10:15:33.522954 1021519 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1203 10:15:33.522979 1021519 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1203 10:15:33.537200 1021519 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/hadi/.minikube/machines/minikube/id_rsa Username:docker}
I1203 10:15:33.615996 1021519 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1203 10:15:33.620008 1021519 fix.go:57] fixHost completed within 7.423537728s
I1203 10:15:33.620019 1021519 start.go:83] releasing machines lock for "minikube", held for 7.423565594s
I1203 10:15:33.620062 1021519 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1203 10:15:33.632386 1021519 ssh_runner.go:195] Run: cat /version.json
I1203 10:15:33.632398 1021519 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1203 10:15:33.632426 1021519 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1203 10:15:33.632431 1021519 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1203 10:15:33.645107 1021519 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/hadi/.minikube/machines/minikube/id_rsa Username:docker}
I1203 10:15:33.646628 1021519 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/hadi/.minikube/machines/minikube/id_rsa Username:docker}
I1203 10:15:34.937251 1021519 ssh_runner.go:235] Completed: cat /version.json: (1.304838876s)
I1203 10:15:34.937424 1021519 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (1.304995436s)
W1203 10:15:34.937467 1021519 out.go:239] ❗  Image was not built for the current minikube version. To resolve this you can delete and recreate your minikube cluster using the latest images. Expected minikube version: v1.29.0 -> Actual minikube version: v1.30.0
W1203 10:15:34.937490 1021519 start.go:830] [curl -sS -m 2 https://registry.k8s.io/] failed: curl -sS -m 2 https://registry.k8s.io/: Process exited with status 7
stdout:

stderr:
curl: (7) Failed to connect to registry.k8s.io port 443: Connection timed out
I1203 10:15:34.937589 1021519 ssh_runner.go:195] Run: systemctl --version
W1203 10:15:34.937660 1021519 out.go:239] ❗  This container is having trouble accessing https://registry.k8s.io
W1203 10:15:34.937696 1021519 out.go:239] 💡  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I1203 10:15:34.948234 1021519 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1203 10:15:34.958855 1021519 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I1203 10:15:34.984382 1021519 cni.go:229] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I1203 10:15:34.984441 1021519 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1203 10:15:34.994543 1021519 cni.go:258] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I1203 10:15:34.994564 1021519 start.go:481] detecting cgroup driver to use...
I1203 10:15:34.994596 1021519 detect.go:199] detected "systemd" cgroup driver on host os
I1203 10:15:34.994692 1021519 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1203 10:15:35.006751 1021519 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I1203 10:15:35.018239 1021519 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1203 10:15:35.028858 1021519 containerd.go:145] configuring containerd to use "systemd" as cgroup driver...
I1203 10:15:35.029067 1021519 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I1203 10:15:35.038348 1021519 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1203 10:15:35.050389 1021519 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1203 10:15:35.062295 1021519 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1203 10:15:35.071155 1021519 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1203 10:15:35.079428 1021519 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1203 10:15:35.086671 1021519 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1203 10:15:35.092324 1021519 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1203 10:15:35.100445 1021519 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1203 10:15:35.160255 1021519 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1203 10:15:35.216219 1021519 start.go:481] detecting cgroup driver to use...
I1203 10:15:35.216335 1021519 detect.go:199] detected "systemd" cgroup driver on host os
I1203 10:15:35.216413 1021519 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1203 10:15:35.230561 1021519 cruntime.go:276] skipping containerd shutdown because we are bound to it
I1203 10:15:35.230604 1021519 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1203 10:15:35.238786 1021519 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1203 10:15:35.254258 1021519 ssh_runner.go:195] Run: which cri-dockerd
I1203 10:15:35.258691 1021519 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1203 10:15:35.268429 1021519 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I1203 10:15:35.281516 1021519 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1203 10:15:35.333442 1021519 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1203 10:15:35.372750 1021519 docker.go:538] configuring docker to use "systemd" as cgroup driver...
I1203 10:15:35.372778 1021519 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (143 bytes)
I1203 10:15:35.384061 1021519 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1203 10:15:35.429311 1021519 ssh_runner.go:195] Run: sudo systemctl restart docker
I1203 10:15:36.637523 1021519 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.208193435s)
I1203 10:15:36.637565 1021519 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1203 10:15:36.688228 1021519 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1203 10:15:36.734750 1021519 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1203 10:15:36.775708 1021519 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1203 10:15:36.817353 1021519 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1203 10:15:36.840277 1021519 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1203 10:15:36.886287 1021519 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I1203 10:15:36.954127 1021519 start.go:528] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1203 10:15:36.954214 1021519 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1203 10:15:36.960230 1021519 start.go:549] Will wait 60s for crictl version
I1203 10:15:36.960271 1021519 ssh_runner.go:195] Run: which crictl
I1203 10:15:36.963279 1021519 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1203 10:15:36.988134 1021519 start.go:565] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  23.0.2
RuntimeApiVersion:  v1alpha2
I1203 10:15:36.988167 1021519 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1203 10:15:37.002526 1021519 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1203 10:15:37.034371 1021519 out.go:204] 🐳  Preparing Kubernetes v1.27.0-rc.0 on Docker 23.0.2 ...
I1203 10:15:37.034582 1021519 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1203 10:15:37.057588 1021519 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I1203 10:15:37.060918 1021519 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1203 10:15:37.069087 1021519 preload.go:132] Checking if preload exists for k8s version v1.27.0-rc.0 and runtime docker
I1203 10:15:37.069135 1021519 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1203 10:15:37.085363 1021519 docker.go:639] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.27.0-rc.0
registry.k8s.io/kube-controller-manager:v1.27.0-rc.0
registry.k8s.io/kube-scheduler:v1.27.0-rc.0
registry.k8s.io/kube-proxy:v1.27.0-rc.0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/etcd:3.5.7-0
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1203 10:15:37.085378 1021519 docker.go:569] Images already preloaded, skipping extraction
I1203 10:15:37.085431 1021519 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1203 10:15:37.101926 1021519 docker.go:639] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.27.0-rc.0
registry.k8s.io/kube-proxy:v1.27.0-rc.0
registry.k8s.io/kube-controller-manager:v1.27.0-rc.0
registry.k8s.io/kube-scheduler:v1.27.0-rc.0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/etcd:3.5.7-0
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1203 10:15:37.101938 1021519 cache_images.go:84] Images are preloaded, skipping loading
I1203 10:15:37.101991 1021519 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1203 10:15:37.121190 1021519 cni.go:84] Creating CNI manager for ""
I1203 10:15:37.121217 1021519 cni.go:157] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1203 10:15:37.121229 1021519 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I1203 10:15:37.121258 1021519 kubeadm.go:172] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.27.0-rc.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m]}
I1203 10:15:37.121455 1021519 kubeadm.go:177] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: /var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.27.0-rc.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1203 10:15:37.121600 1021519 kubeadm.go:968] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.27.0-rc.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.27.0-rc.0 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I1203 10:15:37.121663 1021519 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.27.0-rc.0
I1203 10:15:37.129205 1021519 binaries.go:44] Found k8s binaries, skipping transfer
I1203 10:15:37.129244 1021519 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1203 10:15:37.135148 1021519 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (374 bytes)
I1203 10:15:37.156206 1021519 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (357 bytes)
I1203 10:15:37.169816 1021519 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2088 bytes)
I1203 10:15:37.182033 1021519 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1203 10:15:37.184487 1021519 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1203 10:15:37.191538 1021519 certs.go:56] Setting up /home/hadi/.minikube/profiles/minikube for IP: 192.168.49.2
I1203 10:15:37.191552 1021519 certs.go:186] acquiring lock for shared ca certs: {Name:mk02fc2c6e60291ee4f8e3bdbca5bac82a38045b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1203 10:15:37.191640 1021519 certs.go:195] skipping minikubeCA CA generation: /home/hadi/.minikube/ca.key
I1203 10:15:37.191658 1021519 certs.go:195] skipping proxyClientCA CA generation: /home/hadi/.minikube/proxy-client-ca.key
I1203 10:15:37.191702 1021519 certs.go:311] skipping minikube-user signed cert generation: /home/hadi/.minikube/profiles/minikube/client.key
I1203 10:15:37.191722 1021519 certs.go:311] skipping minikube signed cert generation: /home/hadi/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I1203 10:15:37.191738 1021519 certs.go:311] skipping aggregator signed cert generation: /home/hadi/.minikube/profiles/minikube/proxy-client.key
I1203 10:15:37.191783 1021519 certs.go:401] found cert: /home/hadi/.minikube/certs/home/hadi/.minikube/certs/ca-key.pem (1675 bytes)
I1203 10:15:37.191794 1021519 certs.go:401] found cert: /home/hadi/.minikube/certs/home/hadi/.minikube/certs/ca.pem (1070 bytes)
I1203 10:15:37.191805 1021519 certs.go:401] found cert: /home/hadi/.minikube/certs/home/hadi/.minikube/certs/cert.pem (1115 bytes)
I1203 10:15:37.191815 1021519 certs.go:401] found cert: /home/hadi/.minikube/certs/home/hadi/.minikube/certs/key.pem (1679 bytes)
I1203 10:15:37.192137 1021519 ssh_runner.go:362] scp /home/hadi/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I1203 10:15:37.207872 1021519 ssh_runner.go:362] scp /home/hadi/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I1203 10:15:37.225141 1021519 ssh_runner.go:362] scp /home/hadi/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1203 10:15:37.242192 1021519 ssh_runner.go:362] scp /home/hadi/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I1203 10:15:37.262764 1021519 ssh_runner.go:362] scp /home/hadi/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1203 10:15:37.279402 1021519 ssh_runner.go:362] scp /home/hadi/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1203 10:15:37.299863 1021519 ssh_runner.go:362] scp /home/hadi/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1203 10:15:37.317295 1021519 ssh_runner.go:362] scp /home/hadi/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I1203 10:15:37.333661 1021519 ssh_runner.go:362] scp /home/hadi/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1203 10:15:37.354493 1021519 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (742 bytes)
I1203 10:15:37.368253 1021519 ssh_runner.go:195] Run: openssl version
I1203 10:15:37.372642 1021519 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1203 10:15:37.383336 1021519 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1203 10:15:37.387680 1021519 certs.go:444] hashing: -rw-r--r-- 1 root root 1111 Nov 24 07:15 /usr/share/ca-certificates/minikubeCA.pem
I1203 10:15:37.387726 1021519 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1203 10:15:37.397126 1021519 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1203 10:15:37.412358 1021519 kubeadm.go:401] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase-builds:v0.0.38-1680381266-16207@sha256:426ee3dccdda8a0d40cd86fbdbe440858176d8d4d9c37319b1c702ef226aea93 Memory:3700 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.0-rc.0 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.0-rc.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/hadi:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP:}
I1203 10:15:37.412456 1021519 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1203 10:15:37.429830 1021519 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1203 10:15:37.440573 1021519 kubeadm.go:416] found existing configuration files, will attempt cluster restart
I1203 10:15:37.440580 1021519 kubeadm.go:633] restartCluster start
I1203 10:15:37.440616 1021519 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1203 10:15:37.451373 1021519 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1203 10:15:37.452115 1021519 kubeconfig.go:135] verify returned: extract IP: "minikube" does not appear in /home/hadi/.kube/config
I1203 10:15:37.453104 1021519 kubeconfig.go:146] "minikube" context is missing from /home/hadi/.kube/config - will repair!
I1203 10:15:37.454577 1021519 lock.go:35] WriteFile acquiring /home/hadi/.kube/config: {Name:mk505776fca089b2a8fa8b986216cbe993a1b9ab Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1203 10:15:37.455907 1021519 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1203 10:15:37.462512 1021519 api_server.go:165] Checking apiserver status ...
I1203 10:15:37.462552 1021519 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1203 10:15:37.472721 1021519 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1203 10:15:37.973666 1021519 api_server.go:165] Checking apiserver status ...
I1203 10:15:37.973706 1021519 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1203 10:15:37.984523 1021519 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1203 10:15:38.473258 1021519 api_server.go:165] Checking apiserver status ...
I1203 10:15:38.473314 1021519 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1203 10:15:38.483244 1021519 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1203 10:15:38.973774 1021519 api_server.go:165] Checking apiserver status ...
I1203 10:15:38.974086 1021519 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1203 10:15:38.993903 1021519 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1203 10:15:39.473480 1021519 api_server.go:165] Checking apiserver status ...
I1203 10:15:39.473589 1021519 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1203 10:15:39.485025 1021519 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1203 10:15:39.973292 1021519 api_server.go:165] Checking apiserver status ...
I1203 10:15:39.973555 1021519 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1203 10:15:39.988496 1021519 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1203 10:15:40.473642 1021519 api_server.go:165] Checking apiserver status ...
I1203 10:15:40.473698 1021519 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1203 10:15:40.483857 1021519 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1203 10:15:40.973275 1021519 api_server.go:165] Checking apiserver status ...
I1203 10:15:40.973316 1021519 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1203 10:15:40.983591 1021519 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1203 10:15:41.473511 1021519 api_server.go:165] Checking apiserver status ...
I1203 10:15:41.473562 1021519 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1203 10:15:41.482700 1021519 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1203 10:15:41.973202 1021519 api_server.go:165] Checking apiserver status ...
I1203 10:15:41.973318 1021519 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1203 10:15:41.989109 1021519 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1203 10:15:42.473152 1021519 api_server.go:165] Checking apiserver status ...
I1203 10:15:42.473260 1021519 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1203 10:15:42.490838 1021519 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1203 10:15:42.973258 1021519 api_server.go:165] Checking apiserver status ...
I1203 10:15:42.973334 1021519 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1203 10:15:42.988704 1021519 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1203 10:15:43.473568 1021519 api_server.go:165] Checking apiserver status ...
I1203 10:15:43.473631 1021519 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1203 10:15:43.486909 1021519 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1203 10:15:43.973435 1021519 api_server.go:165] Checking apiserver status ...
I1203 10:15:43.973486 1021519 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1203 10:15:43.983452 1021519 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1203 10:15:44.473183 1021519 api_server.go:165] Checking apiserver status ...
I1203 10:15:44.473286 1021519 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1203 10:15:44.487475 1021519 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1203 10:15:44.973112 1021519 api_server.go:165] Checking apiserver status ...
I1203 10:15:44.973221 1021519 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1203 10:15:44.987614 1021519 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1203 10:15:45.473463 1021519 api_server.go:165] Checking apiserver status ...
I1203 10:15:45.473539 1021519 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1203 10:15:45.487082 1021519 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1203 10:15:45.972845 1021519 api_server.go:165] Checking apiserver status ...
I1203 10:15:45.972895 1021519 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1203 10:15:45.980888 1021519 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1203 10:15:46.473749 1021519 api_server.go:165] Checking apiserver status ...
I1203 10:15:46.473810 1021519 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1203 10:15:46.485204 1021519 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1203 10:15:46.973240 1021519 api_server.go:165] Checking apiserver status ...
I1203 10:15:46.973325 1021519 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1203 10:15:46.981285 1021519 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1203 10:15:47.473746 1021519 api_server.go:165] Checking apiserver status ...
I1203 10:15:47.473843 1021519 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1203 10:15:47.490811 1021519 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1203 10:15:47.490818 1021519 api_server.go:165] Checking apiserver status ...
I1203 10:15:47.490845 1021519 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1203 10:15:47.501314 1021519 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1203 10:15:47.501326 1021519 kubeadm.go:608] needs reconfigure: apiserver error: timed out waiting for the condition
I1203 10:15:47.501330 1021519 kubeadm.go:1120] stopping kube-system containers ...
I1203 10:15:47.501371 1021519 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1203 10:15:47.521474 1021519 docker.go:465] Stopping containers: [640f5a8bfc64 2c49f59c2230 f1dfabf5b507 228e99374b91 7466e5c9e380 9af3deeacd8b df3ffe7e974a 547a9b5e3360 00c7c5c840fe 95012be6d398 21b77cc84376 95ff0793714a a4b4c00f2974 c418e9861fe4 d8a5bb87756d 94051172f5a7 2a3f34b1845e e3bd1585584e b0dc8e10d9a9 919d0013d06b 9c160349064d de62ab2674b3 a61f09954cb5 4adc13b7aa2c 3858f9bb3632 9ed105c2f346 1c15937b14cb]
I1203 10:15:47.521509 1021519 ssh_runner.go:195] Run: docker stop 640f5a8bfc64 2c49f59c2230 f1dfabf5b507 228e99374b91 7466e5c9e380 9af3deeacd8b df3ffe7e974a 547a9b5e3360 00c7c5c840fe 95012be6d398 21b77cc84376 95ff0793714a a4b4c00f2974 c418e9861fe4 d8a5bb87756d 94051172f5a7 2a3f34b1845e e3bd1585584e b0dc8e10d9a9 919d0013d06b 9c160349064d de62ab2674b3 a61f09954cb5 4adc13b7aa2c 3858f9bb3632 9ed105c2f346 1c15937b14cb
I1203 10:15:47.546654 1021519 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I1203 10:15:47.554559 1021519 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1203 10:15:47.559475 1021519 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5643 Dec  3 05:26 /etc/kubernetes/admin.conf
-rw------- 1 root root 5656 Dec  3 05:39 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Dec  3 05:26 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5604 Dec  3 05:39 /etc/kubernetes/scheduler.conf

I1203 10:15:47.559505 1021519 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I1203 10:15:47.565247 1021519 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I1203 10:15:47.571144 1021519 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I1203 10:15:47.577149 1021519 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I1203 10:15:47.577190 1021519 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I1203 10:15:47.584011 1021519 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I1203 10:15:47.590218 1021519 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I1203 10:15:47.590261 1021519 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I1203 10:15:47.599396 1021519 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1203 10:15:47.605873 1021519 kubeadm.go:710] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I1203 10:15:47.605917 1021519 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.0-rc.0:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I1203 10:15:47.643460 1021519 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.0-rc.0:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I1203 10:15:48.106032 1021519 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.0-rc.0:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I1203 10:15:48.211272 1021519 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.0-rc.0:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I1203 10:15:48.248759 1021519 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.0-rc.0:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I1203 10:15:48.285661 1021519 api_server.go:51] waiting for apiserver process to appear ...
I1203 10:15:48.285736 1021519 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1203 10:15:48.800549 1021519 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1203 10:15:49.300354 1021519 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1203 10:15:49.307725 1021519 api_server.go:71] duration metric: took 1.022075207s to wait for apiserver process to appear ...
I1203 10:15:49.307736 1021519 api_server.go:87] waiting for apiserver healthz status ...
I1203 10:15:49.307742 1021519 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1203 10:15:51.343508 1021519 api_server.go:278] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[-]etcd failed: reason withheld
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1203 10:15:51.343523 1021519 api_server.go:102] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[-]etcd failed: reason withheld
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1203 10:15:51.843932 1021519 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1203 10:15:51.856373 1021519 api_server.go:278] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1203 10:15:51.856397 1021519 api_server.go:102] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1203 10:15:52.344281 1021519 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1203 10:15:52.350636 1021519 api_server.go:278] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1203 10:15:52.350670 1021519 api_server.go:102] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1203 10:15:52.844304 1021519 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1203 10:15:52.849592 1021519 api_server.go:278] https://192.168.49.2:8443/healthz returned 200:
ok
I1203 10:15:52.858713 1021519 api_server.go:140] control plane version: v1.27.0-rc.0
I1203 10:15:52.858725 1021519 api_server.go:130] duration metric: took 3.550985159s to wait for apiserver health ...
I1203 10:15:52.858737 1021519 cni.go:84] Creating CNI manager for ""
I1203 10:15:52.858749 1021519 cni.go:157] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1203 10:15:52.864709 1021519 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I1203 10:15:52.868964 1021519 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I1203 10:15:52.879340 1021519 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I1203 10:15:52.899517 1021519 system_pods.go:43] waiting for kube-system pods to appear ...
I1203 10:15:52.906631 1021519 system_pods.go:59] 7 kube-system pods found
I1203 10:15:52.906646 1021519 system_pods.go:61] "coredns-5d78c9869d-6ktmx" [d8bb4f4a-3dc3-4ea5-9cd1-4297c4cba4b2] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1203 10:15:52.906651 1021519 system_pods.go:61] "etcd-minikube" [d5bc9f51-fcde-46de-90f0-5563393d3b7b] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1203 10:15:52.906654 1021519 system_pods.go:61] "kube-apiserver-minikube" [3fdec3e5-edca-4de6-9fad-25174774b8d3] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1203 10:15:52.906657 1021519 system_pods.go:61] "kube-controller-manager-minikube" [5759be76-3a28-49df-867a-706c53e996b4] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I1203 10:15:52.906660 1021519 system_pods.go:61] "kube-proxy-7q8x8" [8eca1a8a-d303-4f98-a4db-d2161f6331be] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I1203 10:15:52.906663 1021519 system_pods.go:61] "kube-scheduler-minikube" [ce35f3f6-9c7b-4d93-a7f4-15a4b4da6dc4] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1203 10:15:52.906665 1021519 system_pods.go:61] "storage-provisioner" [ecdd73f1-24fa-422a-9a52-5e97cdaf625b] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1203 10:15:52.906669 1021519 system_pods.go:74] duration metric: took 7.052308ms to wait for pod list to return data ...
I1203 10:15:52.906673 1021519 node_conditions.go:102] verifying NodePressure condition ...
I1203 10:15:52.910093 1021519 node_conditions.go:122] node storage ephemeral capacity is 128425252Ki
I1203 10:15:52.910105 1021519 node_conditions.go:123] node cpu capacity is 12
I1203 10:15:52.910113 1021519 node_conditions.go:105] duration metric: took 3.437819ms to run NodePressure ...
I1203 10:15:52.910127 1021519 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.0-rc.0:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I1203 10:15:53.083198 1021519 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1203 10:15:53.090907 1021519 ops.go:34] apiserver oom_adj: -16
I1203 10:15:53.090914 1021519 kubeadm.go:637] restartCluster took 15.650330915s
I1203 10:15:53.090919 1021519 kubeadm.go:403] StartCluster complete in 15.678570834s
I1203 10:15:53.090931 1021519 settings.go:142] acquiring lock: {Name:mk3a4cf653888ac560be40255d40906649e2121c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1203 10:15:53.090979 1021519 settings.go:150] Updating kubeconfig:  /home/hadi/.kube/config
I1203 10:15:53.092178 1021519 lock.go:35] WriteFile acquiring /home/hadi/.kube/config: {Name:mk505776fca089b2a8fa8b986216cbe993a1b9ab Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1203 10:15:53.092388 1021519 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.27.0-rc.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1203 10:15:53.092451 1021519 addons.go:496] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false]
I1203 10:15:53.092515 1021519 addons.go:66] Setting storage-provisioner=true in profile "minikube"
I1203 10:15:53.092522 1021519 addons.go:66] Setting default-storageclass=true in profile "minikube"
I1203 10:15:53.092536 1021519 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1203 10:15:53.092538 1021519 addons.go:228] Setting addon storage-provisioner=true in "minikube"
W1203 10:15:53.092543 1021519 addons.go:237] addon storage-provisioner should already be in state true
I1203 10:15:53.092588 1021519 host.go:66] Checking if "minikube" exists ...
I1203 10:15:53.092600 1021519 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.0-rc.0
I1203 10:15:53.092791 1021519 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1203 10:15:53.092931 1021519 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1203 10:15:53.097694 1021519 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I1203 10:15:53.097736 1021519 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.0-rc.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1203 10:15:53.106124 1021519 out.go:177] 🔎  Verifying Kubernetes components...
I1203 10:15:53.110328 1021519 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I1203 10:15:53.128630 1021519 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1203 10:15:53.133957 1021519 addons.go:420] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1203 10:15:53.133974 1021519 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1203 10:15:53.134175 1021519 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1203 10:15:53.140592 1021519 addons.go:228] Setting addon default-storageclass=true in "minikube"
W1203 10:15:53.140608 1021519 addons.go:237] addon default-storageclass should already be in state true
I1203 10:15:53.140637 1021519 host.go:66] Checking if "minikube" exists ...
I1203 10:15:53.141198 1021519 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1203 10:15:53.153860 1021519 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/hadi/.minikube/machines/minikube/id_rsa Username:docker}
I1203 10:15:53.158452 1021519 addons.go:420] installing /etc/kubernetes/addons/storageclass.yaml
I1203 10:15:53.158465 1021519 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1203 10:15:53.158507 1021519 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1203 10:15:53.173372 1021519 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/hadi/.minikube/machines/minikube/id_rsa Username:docker}
I1203 10:15:53.181558 1021519 api_server.go:51] waiting for apiserver process to appear ...
I1203 10:15:53.181558 1021519 start.go:889] CoreDNS already contains "host.minikube.internal" host record, skipping...
I1203 10:15:53.181589 1021519 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1203 10:15:53.188702 1021519 api_server.go:71] duration metric: took 90.924992ms to wait for apiserver process to appear ...
I1203 10:15:53.188714 1021519 api_server.go:87] waiting for apiserver healthz status ...
I1203 10:15:53.188722 1021519 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1203 10:15:53.192582 1021519 api_server.go:278] https://192.168.49.2:8443/healthz returned 200:
ok
I1203 10:15:53.193112 1021519 api_server.go:140] control plane version: v1.27.0-rc.0
I1203 10:15:53.193118 1021519 api_server.go:130] duration metric: took 4.401465ms to wait for apiserver health ...
I1203 10:15:53.193122 1021519 system_pods.go:43] waiting for kube-system pods to appear ...
I1203 10:15:53.196476 1021519 system_pods.go:59] 7 kube-system pods found
I1203 10:15:53.196486 1021519 system_pods.go:61] "coredns-5d78c9869d-6ktmx" [d8bb4f4a-3dc3-4ea5-9cd1-4297c4cba4b2] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1203 10:15:53.196493 1021519 system_pods.go:61] "etcd-minikube" [d5bc9f51-fcde-46de-90f0-5563393d3b7b] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1203 10:15:53.196496 1021519 system_pods.go:61] "kube-apiserver-minikube" [3fdec3e5-edca-4de6-9fad-25174774b8d3] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1203 10:15:53.196499 1021519 system_pods.go:61] "kube-controller-manager-minikube" [5759be76-3a28-49df-867a-706c53e996b4] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I1203 10:15:53.196502 1021519 system_pods.go:61] "kube-proxy-7q8x8" [8eca1a8a-d303-4f98-a4db-d2161f6331be] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I1203 10:15:53.196505 1021519 system_pods.go:61] "kube-scheduler-minikube" [ce35f3f6-9c7b-4d93-a7f4-15a4b4da6dc4] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1203 10:15:53.196507 1021519 system_pods.go:61] "storage-provisioner" [ecdd73f1-24fa-422a-9a52-5e97cdaf625b] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1203 10:15:53.196510 1021519 system_pods.go:74] duration metric: took 3.385454ms to wait for pod list to return data ...
I1203 10:15:53.196514 1021519 kubeadm.go:578] duration metric: took 98.743046ms to wait for : map[apiserver:true system_pods:true] ...
I1203 10:15:53.196521 1021519 node_conditions.go:102] verifying NodePressure condition ...
I1203 10:15:53.198501 1021519 node_conditions.go:122] node storage ephemeral capacity is 128425252Ki
I1203 10:15:53.198508 1021519 node_conditions.go:123] node cpu capacity is 12
I1203 10:15:53.198513 1021519 node_conditions.go:105] duration metric: took 1.989997ms to run NodePressure ...
I1203 10:15:53.198520 1021519 start.go:228] waiting for startup goroutines ...
I1203 10:15:53.239792 1021519 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.0-rc.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1203 10:15:53.260264 1021519 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.0-rc.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
W1203 10:15:53.619370 1021519 addons.go:446] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.0-rc.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
Unable to connect to the server: stream error: stream ID 1; INTERNAL_ERROR; received from peer
I1203 10:15:53.619406 1021519 retry.go:31] will retry after 203.927151ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.0-rc.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
Unable to connect to the server: stream error: stream ID 1; INTERNAL_ERROR; received from peer
I1203 10:15:53.823819 1021519 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.0-rc.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1203 10:15:53.960538 1021519 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass
I1203 10:15:53.964256 1021519 addons.go:499] enable addons completed in 871.774776ms: enabled=[storage-provisioner default-storageclass]
I1203 10:15:53.964372 1021519 start.go:233] waiting for cluster config update ...
I1203 10:15:53.964400 1021519 start.go:242] writing updated cluster config ...
I1203 10:15:53.964803 1021519 ssh_runner.go:195] Run: rm -f paused
I1203 10:15:54.005664 1021519 start.go:557] kubectl: 1.31.2, cluster: 1.27.0-rc.0 (minor skew: 4)
I1203 10:15:54.009808 1021519 out.go:177] 
W1203 10:15:54.013689 1021519 out.go:239] ❗  /usr/local/bin/kubectl is version 1.31.2, which may have incompatibilities with Kubernetes 1.27.0-rc.0.
I1203 10:15:54.017586 1021519 out.go:177]     ▪ Want kubectl v1.27.0-rc.0? Try 'minikube kubectl -- get pods -A'
I1203 10:15:54.025085 1021519 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* -- Logs begin at Tue 2024-12-03 05:45:31 UTC, end at Tue 2024-12-03 07:28:15 UTC. --
Dec 03 05:50:23 minikube dockerd[947]: time="2024-12-03T05:50:23.330782068Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": dial tcp 54.227.20.253:443: i/o timeout"
Dec 03 05:50:23 minikube dockerd[947]: time="2024-12-03T05:50:23.330861616Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": dial tcp 54.227.20.253:443: i/o timeout"
Dec 03 05:50:23 minikube dockerd[947]: time="2024-12-03T05:50:23.347705109Z" level=error msg="Handler for POST /v1.41/images/create returned error: Get \"https://registry-1.docker.io/v2/\": dial tcp 54.227.20.253:443: i/o timeout"
Dec 03 05:50:33 minikube dockerd[947]: time="2024-12-03T05:50:33.126096148Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Dec 03 05:50:33 minikube dockerd[947]: time="2024-12-03T05:50:33.126148904Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Dec 03 05:50:33 minikube dockerd[947]: time="2024-12-03T05:50:33.130813306Z" level=error msg="Handler for POST /v1.42/images/create returned error: Get \"https://registry-1.docker.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Dec 03 05:52:08 minikube dockerd[947]: time="2024-12-03T05:52:08.317046386Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)"
Dec 03 05:52:08 minikube dockerd[947]: time="2024-12-03T05:52:08.317124657Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)"
Dec 03 05:52:08 minikube dockerd[947]: time="2024-12-03T05:52:08.331453486Z" level=error msg="Handler for POST /v1.41/images/create returned error: Get \"https://registry-1.docker.io/v2/\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)"
Dec 03 05:54:59 minikube dockerd[947]: time="2024-12-03T05:54:59.607014556Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io: Temporary failure in name resolution"
Dec 03 05:54:59 minikube dockerd[947]: time="2024-12-03T05:54:59.607198665Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io: Temporary failure in name resolution"
Dec 03 05:54:59 minikube dockerd[947]: time="2024-12-03T05:54:59.613162953Z" level=error msg="Handler for POST /v1.41/images/create returned error: Get \"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io: Temporary failure in name resolution"
Dec 03 06:02:06 minikube cri-dockerd[1179]: time="2024-12-03T06:02:06Z" level=info msg="Pulling image hadirezaie/kubernetes-springboot:latest: cf11b3ab5dab: Downloading [=================================>                 ]  13.86MB/20.86MB"
Dec 03 06:02:16 minikube cri-dockerd[1179]: time="2024-12-03T06:02:16Z" level=info msg="Pulling image hadirezaie/kubernetes-springboot:latest: cf11b3ab5dab: Downloading [=================================>                 ]  14.07MB/20.86MB"
Dec 03 06:02:26 minikube cri-dockerd[1179]: time="2024-12-03T06:02:26Z" level=info msg="Pulling image hadirezaie/kubernetes-springboot:latest: cf11b3ab5dab: Downloading [===================================>               ]  14.71MB/20.86MB"
Dec 03 06:02:36 minikube cri-dockerd[1179]: time="2024-12-03T06:02:36Z" level=info msg="Pulling image hadirezaie/kubernetes-springboot:latest: cf11b3ab5dab: Downloading [====================================>              ]  15.35MB/20.86MB"
Dec 03 06:02:46 minikube cri-dockerd[1179]: time="2024-12-03T06:02:46Z" level=info msg="Pulling image hadirezaie/kubernetes-springboot:latest: cf11b3ab5dab: Downloading [======================================>            ]  15.99MB/20.86MB"
Dec 03 06:02:56 minikube cri-dockerd[1179]: time="2024-12-03T06:02:56Z" level=info msg="Pulling image hadirezaie/kubernetes-springboot:latest: cf11b3ab5dab: Downloading [=======================================>           ]  16.41MB/20.86MB"
Dec 03 06:03:06 minikube cri-dockerd[1179]: time="2024-12-03T06:03:06Z" level=info msg="Pulling image hadirezaie/kubernetes-springboot:latest: cf11b3ab5dab: Downloading [========================================>          ]  17.05MB/20.86MB"
Dec 03 06:03:16 minikube cri-dockerd[1179]: time="2024-12-03T06:03:16Z" level=info msg="Pulling image hadirezaie/kubernetes-springboot:latest: cf11b3ab5dab: Downloading [=========================================>         ]  17.48MB/20.86MB"
Dec 03 06:03:26 minikube cri-dockerd[1179]: time="2024-12-03T06:03:26Z" level=info msg="Pulling image hadirezaie/kubernetes-springboot:latest: cf11b3ab5dab: Downloading [===========================================>       ]  18.12MB/20.86MB"
Dec 03 06:03:36 minikube cri-dockerd[1179]: time="2024-12-03T06:03:36Z" level=info msg="Pulling image hadirezaie/kubernetes-springboot:latest: cf11b3ab5dab: Downloading [============================================>      ]  18.76MB/20.86MB"
Dec 03 06:03:46 minikube cri-dockerd[1179]: time="2024-12-03T06:03:46Z" level=info msg="Pulling image hadirezaie/kubernetes-springboot:latest: cf11b3ab5dab: Downloading [=============================================>     ]  19.18MB/20.86MB"
Dec 03 06:03:56 minikube cri-dockerd[1179]: time="2024-12-03T06:03:56Z" level=info msg="Pulling image hadirezaie/kubernetes-springboot:latest: cf11b3ab5dab: Downloading [===============================================>   ]  19.82MB/20.86MB"
Dec 03 06:04:06 minikube cri-dockerd[1179]: time="2024-12-03T06:04:06Z" level=info msg="Stop pulling image hadirezaie/kubernetes-springboot:latest: Status: Downloaded newer image for hadirezaie/kubernetes-springboot:latest"
Dec 03 06:04:34 minikube cri-dockerd[1179]: time="2024-12-03T06:04:34Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/42330a29bbc0f15ae52f3e0370c86b1678ed0eb27d6492488ca5cce5cf76e180/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 03 06:04:34 minikube cri-dockerd[1179]: time="2024-12-03T06:04:34Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/cc2e4535dd639445d185f395e8c1d27263eb5b89b40cdc249381459d6c611c16/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 03 06:04:35 minikube cri-dockerd[1179]: time="2024-12-03T06:04:35Z" level=error msg="Error adding pod kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747-2xp5s to network {docker cc2e4535dd639445d185f395e8c1d27263eb5b89b40cdc249381459d6c611c16}:/proc/17711/ns/net:bridge:bridge: plugin type=\"bridge\" failed (add): failed to list chains: running [/usr/sbin/iptables -t nat -S --wait]: exit status -1: "
Dec 03 06:04:35 minikube cri-dockerd[1179]: time="2024-12-03T06:04:35Z" level=error msg="Error deleting pod kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747-2xp5s from network {docker cc2e4535dd639445d185f395e8c1d27263eb5b89b40cdc249381459d6c611c16}:/proc/17711/ns/net:bridge:bridge: plugin type=\"bridge\" failed (delete): running [/usr/sbin/iptables -t nat -D POSTROUTING -s 10.244.0.9 -j CNI-539cb4589c7468b490f46c2a -m comment --comment name: \"bridge\" id: \"cc2e4535dd639445d185f395e8c1d27263eb5b89b40cdc249381459d6c611c16\" --wait]: exit status 2: iptables v1.8.4 (nf_tables): Chain 'CNI-539cb4589c7468b490f46c2a' does not exist\nTry `iptables -h' or 'iptables --help' for more information.\n"
Dec 03 06:04:35 minikube dockerd[947]: time="2024-12-03T06:04:35.150426616Z" level=info msg="ignoring event" container=cc2e4535dd639445d185f395e8c1d27263eb5b89b40cdc249381459d6c611c16 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 03 06:04:35 minikube cri-dockerd[1179]: time="2024-12-03T06:04:35Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"kubernetes-dashboard-5c5cfc8747-2xp5s_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"cc2e4535dd639445d185f395e8c1d27263eb5b89b40cdc249381459d6c611c16\""
Dec 03 06:04:35 minikube cri-dockerd[1179]: time="2024-12-03T06:04:35Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a76c433f0106206ba29c40d73523779503ded712c9d22707d48e6efa3ba1a4eb/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 03 06:04:35 minikube dockerd[947]: time="2024-12-03T06:04:35.853796922Z" level=warning msg="reference for unknown type: " digest="sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c" remote="docker.io/kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c"
Dec 03 06:04:46 minikube cri-dockerd[1179]: time="2024-12-03T06:04:46Z" level=info msg="Stop pulling image docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c: Status: Downloaded newer image for kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c"
Dec 03 06:04:47 minikube dockerd[947]: time="2024-12-03T06:04:47.005090164Z" level=warning msg="reference for unknown type: " digest="sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93" remote="docker.io/kubernetesui/dashboard@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93"
Dec 03 06:04:58 minikube cri-dockerd[1179]: time="2024-12-03T06:04:58Z" level=info msg="Pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: ee3247c7e545: Downloading [==================>                                ]  28.06MB/75.78MB"
Dec 03 06:05:08 minikube cri-dockerd[1179]: time="2024-12-03T06:05:08Z" level=info msg="Pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: ee3247c7e545: Downloading [======================================>            ]  57.72MB/75.78MB"
Dec 03 06:05:17 minikube cri-dockerd[1179]: time="2024-12-03T06:05:17Z" level=info msg="Stop pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: Status: Downloaded newer image for kubernetesui/dashboard@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93"
Dec 03 06:23:55 minikube cri-dockerd[1179]: time="2024-12-03T06:23:55Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a3c9e6c1b567597d7df215a171f00b9c474bb479cb7d6c4bfd15f747b19df157/resolv.conf as [nameserver 10.96.0.10 search kubernetes-springboot.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 03 06:23:58 minikube cri-dockerd[1179]: time="2024-12-03T06:23:58Z" level=info msg="Stop pulling image hadirezaie/kubernetes-springboot:latest: Status: Image is up to date for hadirezaie/kubernetes-springboot:latest"
Dec 03 06:23:59 minikube dockerd[947]: time="2024-12-03T06:23:59.894352305Z" level=info msg="ignoring event" container=85b041caf871eda1fab449c40ddc552e65cf1e156adbf7e969b517587895d6bb module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 03 06:24:00 minikube dockerd[947]: time="2024-12-03T06:24:00.124010508Z" level=info msg="ignoring event" container=c580ff7dbc2c8e2ba50722c295b22b10f5b40e22eada19dd8ddb634ed1aa7e25 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 03 07:15:11 minikube cri-dockerd[1179]: time="2024-12-03T07:15:11Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f58411e3d64e0833c492c8edbc9d8afba88abc879be5a69e08f58415bf3cc5d0/resolv.conf as [nameserver 10.96.0.10 search kube-system.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 03 07:15:26 minikube dockerd[947]: time="2024-12-03T07:15:26.406961403Z" level=warning msg="Error getting v2 registry: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Dec 03 07:15:26 minikube dockerd[947]: time="2024-12-03T07:15:26.407063715Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Dec 03 07:15:26 minikube dockerd[947]: time="2024-12-03T07:15:26.420404345Z" level=error msg="Handler for POST /v1.41/images/create returned error: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Dec 03 07:15:55 minikube dockerd[947]: time="2024-12-03T07:15:55.036075534Z" level=warning msg="Error getting v2 registry: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Dec 03 07:15:55 minikube dockerd[947]: time="2024-12-03T07:15:55.036153776Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Dec 03 07:15:55 minikube dockerd[947]: time="2024-12-03T07:15:55.054140451Z" level=error msg="Handler for POST /v1.41/images/create returned error: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Dec 03 07:16:36 minikube dockerd[947]: time="2024-12-03T07:16:36.062151365Z" level=warning msg="Error getting v2 registry: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Dec 03 07:16:36 minikube dockerd[947]: time="2024-12-03T07:16:36.062240486Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Dec 03 07:16:36 minikube dockerd[947]: time="2024-12-03T07:16:36.076946918Z" level=error msg="Handler for POST /v1.41/images/create returned error: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Dec 03 07:17:38 minikube dockerd[947]: time="2024-12-03T07:17:38.049203443Z" level=warning msg="Error getting v2 registry: Get \"https://k8s.gcr.io/v2/\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)"
Dec 03 07:17:38 minikube dockerd[947]: time="2024-12-03T07:17:38.049262971Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://k8s.gcr.io/v2/\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)"
Dec 03 07:17:38 minikube dockerd[947]: time="2024-12-03T07:17:38.063356919Z" level=error msg="Handler for POST /v1.41/images/create returned error: Get \"https://k8s.gcr.io/v2/\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)"
Dec 03 07:19:11 minikube dockerd[947]: time="2024-12-03T07:19:11.388072453Z" level=warning msg="Error getting v2 registry: Get \"https://k8s.gcr.io/v2/\": dial tcp 66.102.1.82:443: connect: no route to host"
Dec 03 07:19:11 minikube dockerd[947]: time="2024-12-03T07:19:11.388292958Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://k8s.gcr.io/v2/\": dial tcp 66.102.1.82:443: connect: no route to host"
Dec 03 07:19:11 minikube dockerd[947]: time="2024-12-03T07:19:11.395778505Z" level=error msg="Handler for POST /v1.41/images/create returned error: Get \"https://k8s.gcr.io/v2/\": dial tcp 66.102.1.82:443: connect: no route to host"
Dec 03 07:23:42 minikube dockerd[947]: time="2024-12-03T07:23:42.663248996Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/metrics-server/metrics-server/manifests/sha256:6c5603956c0aed6b4087a8716afce8eb22f664b13162346ee852b4fab305ca15\": dial tcp [2600:1901:0:bbc4::]:443: connect: network is unreachable"
Dec 03 07:23:42 minikube dockerd[947]: time="2024-12-03T07:23:42.672546165Z" level=error msg="Handler for POST /v1.41/images/create returned error: Get \"https://registry.k8s.io/v2/metrics-server/metrics-server/manifests/sha256:6c5603956c0aed6b4087a8716afce8eb22f664b13162346ee852b4fab305ca15\": dial tcp [2600:1901:0:bbc4::]:443: connect: network is unreachable"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                                      CREATED             STATE               NAME                        ATTEMPT             POD ID
24f05add249f2       hadirezaie/kubernetes-springboot@sha256:1ad5e7c74ee34a8d489879e33924304a14ae6129d8bb1d0c9b4ff2908cdea698   About an hour ago   Running             kubernetes-springboot       0                   a3c9e6c1b5675
1b9529e755ead       kubernetesui/dashboard@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93             About an hour ago   Running             kubernetes-dashboard        0                   a76c433f01062
b9dc78d092d7f       kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c       About an hour ago   Running             dashboard-metrics-scraper   0                   42330a29bbc0f
c28cc19376119       6e38f40d628db                                                                                              2 hours ago         Running             storage-provisioner         4                   c17a157e580bc
5fc33fba53ddc       ead0a4a53df89                                                                                              2 hours ago         Running             coredns                     2                   dc353a57e0041
c599b3ecfb38f       6e38f40d628db                                                                                              2 hours ago         Exited              storage-provisioner         3                   c17a157e580bc
328d43db00760       6020a40247fc8                                                                                              2 hours ago         Running             kube-proxy                  2                   3d753a637f05f
19736605bf9a2       86b6af7dd652c                                                                                              2 hours ago         Running             etcd                        2                   1cb52dfab2578
897c9a4718d42       9f9d741d7f1c5                                                                                              2 hours ago         Running             kube-controller-manager     2                   96bf3601e453a
ec71bfd34b240       2e5f542d09de7                                                                                              2 hours ago         Running             kube-apiserver              2                   8979842f3e80a
79a9d4f6b3670       d468edbd6d11a                                                                                              2 hours ago         Running             kube-scheduler              2                   ef2195eec76d9
f1dfabf5b507f       6020a40247fc8                                                                                              2 hours ago         Exited              kube-proxy                  1                   7466e5c9e3805
228e99374b91f       ead0a4a53df89                                                                                              2 hours ago         Exited              coredns                     1                   df3ffe7e974a1
547a9b5e3360c       d468edbd6d11a                                                                                              2 hours ago         Exited              kube-scheduler              1                   d8a5bb87756d2
00c7c5c840fe4       2e5f542d09de7                                                                                              2 hours ago         Exited              kube-apiserver              1                   a4b4c00f2974f
95012be6d398a       9f9d741d7f1c5                                                                                              2 hours ago         Exited              kube-controller-manager     1                   95ff0793714ac
21b77cc843767       86b6af7dd652c                                                                                              2 hours ago         Exited              etcd                        1                   c418e9861fe45

* 
* ==> coredns [228e99374b91] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:41528 - 49480 "HINFO IN 801222471246914416.1199640720827868725. udp 56 false 512" - - 0 6.003131437s
[ERROR] plugin/errors: 2 801222471246914416.1199640720827868725. HINFO: read udp 10.244.0.5:45059->192.168.49.1:53: i/o timeout
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] 127.0.0.1:36078 - 15946 "HINFO IN 801222471246914416.1199640720827868725. udp 56 false 512" - - 0 6.002070122s
[ERROR] plugin/errors: 2 801222471246914416.1199640720827868725. HINFO: read udp 10.244.0.5:33520->192.168.49.1:53: i/o timeout
[INFO] 127.0.0.1:34173 - 24959 "HINFO IN 801222471246914416.1199640720827868725. udp 56 false 512" NXDOMAIN qr,rd,ra 131 4.173695363s
[INFO] 127.0.0.1:38148 - 5205 "HINFO IN 801222471246914416.1199640720827868725. udp 56 false 512" NXDOMAIN qr,rd,ra 131 1.173268961s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s

* 
* ==> coredns [5fc33fba53dd] <==
* .:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:38004 - 33497 "HINFO IN 1486572258688859171.5071762637980170040. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.178789885s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=ba4594e7b78814fd52a9376decb9c3d59c133712
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_12_03T09_56_51_0700
                    minikube.k8s.io/version=v1.30.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 03 Dec 2024 05:26:47 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Tue, 03 Dec 2024 07:28:15 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 03 Dec 2024 07:25:07 +0000   Tue, 03 Dec 2024 05:26:46 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 03 Dec 2024 07:25:07 +0000   Tue, 03 Dec 2024 05:26:46 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 03 Dec 2024 07:25:07 +0000   Tue, 03 Dec 2024 05:26:46 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 03 Dec 2024 07:25:07 +0000   Tue, 03 Dec 2024 05:26:47 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                12
  ephemeral-storage:  128425252Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             15528672Ki
  pods:               110
Allocatable:
  cpu:                12
  ephemeral-storage:  128425252Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             15528672Ki
  pods:               110
System Info:
  Machine ID:                 36d3aacc54a5484eb1c230b6c41171d5
  System UUID:                f6d2c2b4-37a9-4c99-a085-8d48d7028a66
  Boot ID:                    002d05ee-d8ec-4fdb-ae6f-1d5c88a438be
  Kernel Version:             6.11.0-9-generic
  OS Image:                   Ubuntu 20.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://23.0.2
  Kubelet Version:            v1.27.0-rc.0
  Kube-Proxy Version:         v1.27.0-rc.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (11 in total)
  Namespace                   Name                                                 CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                                 ------------  ----------  ---------------  -------------  ---
  kube-system                 coredns-5d78c9869d-6ktmx                             100m (0%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (1%!)(MISSING)     121m
  kube-system                 etcd-minikube                                        100m (0%!)(MISSING)     0 (0%!)(MISSING)      100Mi (0%!)(MISSING)       0 (0%!)(MISSING)         121m
  kube-system                 kube-apiserver-minikube                              250m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         121m
  kube-system                 kube-controller-manager-minikube                     200m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         121m
  kube-system                 kube-proxy-7q8x8                                     0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         121m
  kube-system                 kube-scheduler-minikube                              100m (0%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         121m
  kube-system                 metrics-server-76f7df5ddd-2mwgj                      100m (0%!)(MISSING)     0 (0%!)(MISSING)      200Mi (1%!)(MISSING)       0 (0%!)(MISSING)         13m
  kube-system                 storage-provisioner                                  0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         121m
  kubernetes-dashboard        dashboard-metrics-scraper-5dd9cbfd69-pkkjt           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         83m
  kubernetes-dashboard        kubernetes-dashboard-5c5cfc8747-2xp5s                0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         83m
  kubernetes-springboot       kubernetes-springboot-deployment-59875ccf8b-2xvxt    100m (0%!)(MISSING)     250m (2%!)(MISSING)   128Mi (0%!)(MISSING)       256Mi (1%!)(MISSING)     64m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                950m (7%!)(MISSING)   250m (2%!)(MISSING)
  memory             498Mi (3%!)(MISSING)  426Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:              <none>

* 
* ==> dmesg <==
* [Dec 2 14:25] iwlwifi 0000:00:14.3: RF_KILL bit toggled to disable radio.
[  +0.000012] iwlwifi 0000:00:14.3: reporting RF_KILL (radio disabled)
[  +1.107575] iwlwifi 0000:00:14.3: RF_KILL bit toggled to enable radio.
[  +0.000015] iwlwifi 0000:00:14.3: reporting RF_KILL (radio enabled)
[Dec 2 14:41] workqueue: set_brightness_delayed hogged CPU for >10000us 7 times, consider switching to WQ_UNBOUND
[Dec 2 15:22] done.
[Dec 2 15:50] workqueue: set_brightness_delayed hogged CPU for >10000us 11 times, consider switching to WQ_UNBOUND
[Dec 2 16:11] workqueue: set_brightness_delayed hogged CPU for >10000us 19 times, consider switching to WQ_UNBOUND
[Dec 2 16:12] kauditd_printk_skb: 127 callbacks suppressed
[Dec 2 16:29] kauditd_printk_skb: 181 callbacks suppressed
[Dec 2 16:30] kauditd_printk_skb: 6 callbacks suppressed
[Dec 2 17:09] workqueue: delayed_fput hogged CPU for >10000us 67 times, consider switching to WQ_UNBOUND
[Dec 2 17:38] kauditd_printk_skb: 125 callbacks suppressed
[ +15.670799] kauditd_printk_skb: 129 callbacks suppressed
[Dec 2 17:53] iwlwifi 0000:00:14.3: RF_KILL bit toggled to disable radio.
[  +0.000041] iwlwifi 0000:00:14.3: reporting RF_KILL (radio disabled)
[  +1.192646] iwlwifi 0000:00:14.3: RF_KILL bit toggled to enable radio.
[  +0.000016] iwlwifi 0000:00:14.3: reporting RF_KILL (radio enabled)
[  +4.197988] iwlwifi 0000:00:14.3: Unhandled alg: 0x707
[  +0.000442] iwlwifi 0000:00:14.3: Unhandled alg: 0x707
[Dec 2 17:57] workqueue: acpi_os_execute_deferred hogged CPU for >10000us 4 times, consider switching to WQ_UNBOUND
[Dec 3 02:15] done.
[Dec 3 02:40] workqueue: set_brightness_delayed hogged CPU for >10000us 35 times, consider switching to WQ_UNBOUND
[Dec 3 02:54] workqueue: acpi_os_execute_deferred hogged CPU for >10000us 5 times, consider switching to WQ_UNBOUND
[Dec 3 03:38] workqueue: acpi_ec_event_processor hogged CPU for >10000us 4 times, consider switching to WQ_UNBOUND
[  +0.016001] workqueue: acpi_os_execute_deferred hogged CPU for >10000us 7 times, consider switching to WQ_UNBOUND
[Dec 3 04:25] workqueue: acpi_ec_event_processor hogged CPU for >10000us 5 times, consider switching to WQ_UNBOUND
[Dec 3 04:26] workqueue: acpi_ec_event_processor hogged CPU for >10000us 7 times, consider switching to WQ_UNBOUND
[Dec 3 04:27] workqueue: acpi_os_execute_deferred hogged CPU for >10000us 11 times, consider switching to WQ_UNBOUND
[  +2.265920] workqueue: acpi_ec_event_processor hogged CPU for >10000us 11 times, consider switching to WQ_UNBOUND
[  +0.012613] typec port0-partner: PM: parent port0 should not be sleeping
[  +0.607995] workqueue: acpi_os_execute_deferred hogged CPU for >10000us 19 times, consider switching to WQ_UNBOUND
[  +0.053996] workqueue: acpi_ec_event_processor hogged CPU for >10000us 19 times, consider switching to WQ_UNBOUND
[  +0.079007] workqueue: ucsi_handle_connector_change [typec_ucsi] hogged CPU for >10000us 4 times, consider switching to WQ_UNBOUND
[  +4.408323] workqueue: ucsi_handle_connector_change [typec_ucsi] hogged CPU for >10000us 5 times, consider switching to WQ_UNBOUND
[  +3.260544] done.
[ +25.972521] kauditd_printk_skb: 337 callbacks suppressed
[  +8.816268] kauditd_printk_skb: 126 callbacks suppressed
[Dec 3 04:47] kauditd_printk_skb: 125 callbacks suppressed
[ +29.249541] kauditd_printk_skb: 127 callbacks suppressed
[Dec 3 05:25] kauditd_printk_skb: 125 callbacks suppressed
[Dec 3 05:26] kauditd_printk_skb: 128 callbacks suppressed
[Dec 3 05:44] kauditd_printk_skb: 1 callbacks suppressed
[Dec 3 05:56] done.
[Dec 3 05:57] kauditd_printk_skb: 127 callbacks suppressed
[  +9.014508] kauditd_printk_skb: 127 callbacks suppressed
[ +11.657133] kauditd_printk_skb: 125 callbacks suppressed
[Dec 3 06:38] typec port0-partner: PM: parent port0 should not be sleeping
[  +0.300481] workqueue: ucsi_handle_connector_change [typec_ucsi] hogged CPU for >10000us 7 times, consider switching to WQ_UNBOUND
[  +0.207970] workqueue: acpi_os_execute_deferred hogged CPU for >10000us 35 times, consider switching to WQ_UNBOUND
[  +0.150348] workqueue: acpi_ec_event_processor hogged CPU for >10000us 35 times, consider switching to WQ_UNBOUND
[  +3.578202] done.
[Dec 3 06:39] kauditd_printk_skb: 256 callbacks suppressed
[  +5.132361] kauditd_printk_skb: 211 callbacks suppressed
[  +7.096213] kauditd_printk_skb: 20 callbacks suppressed
[Dec 3 06:41] kauditd_printk_skb: 127 callbacks suppressed
[Dec 3 06:47] kauditd_printk_skb: 1 callbacks suppressed
[Dec 3 07:20] done.
[Dec 3 07:21] kauditd_printk_skb: 127 callbacks suppressed
[Dec 3 07:23] kauditd_printk_skb: 129 callbacks suppressed

* 
* ==> etcd [19736605bf9a] <==
* {"level":"info","ts":"2024-12-03T05:45:50.503Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-12-03T05:45:50.505Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-12-03T05:45:50.506Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-12-03T05:46:37.410Z","caller":"traceutil/trace.go:171","msg":"trace[2081295936] transaction","detail":"{read_only:false; response_revision:1347; number_of_response:1; }","duration":"102.474052ms","start":"2024-12-03T05:46:37.307Z","end":"2024-12-03T05:46:37.409Z","steps":["trace[2081295936] 'process raft request'  (duration: 100.701823ms)"],"step_count":1}
{"level":"info","ts":"2024-12-03T05:57:31.816Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1597}
{"level":"info","ts":"2024-12-03T05:57:31.837Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":1597,"took":"19.99698ms","hash":1362419209}
{"level":"info","ts":"2024-12-03T05:57:31.837Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1362419209,"revision":1597,"compact-revision":604}
{"level":"info","ts":"2024-12-03T06:02:31.839Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1841}
{"level":"info","ts":"2024-12-03T06:02:31.858Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":1841,"took":"18.767736ms","hash":2049987249}
{"level":"info","ts":"2024-12-03T06:02:31.858Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2049987249,"revision":1841,"compact-revision":1597}
{"level":"info","ts":"2024-12-03T06:07:31.853Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2081}
{"level":"info","ts":"2024-12-03T06:07:31.863Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":2081,"took":"7.667061ms","hash":1137546215}
{"level":"info","ts":"2024-12-03T06:07:31.863Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1137546215,"revision":2081,"compact-revision":1841}
{"level":"info","ts":"2024-12-03T06:12:31.877Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2412}
{"level":"info","ts":"2024-12-03T06:12:31.884Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":2412,"took":"6.591698ms","hash":4270381187}
{"level":"info","ts":"2024-12-03T06:12:31.884Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4270381187,"revision":2412,"compact-revision":2081}
{"level":"info","ts":"2024-12-03T06:17:31.890Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2651}
{"level":"info","ts":"2024-12-03T06:17:31.893Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":2651,"took":"2.232039ms","hash":41986679}
{"level":"info","ts":"2024-12-03T06:17:31.893Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":41986679,"revision":2651,"compact-revision":2412}
{"level":"info","ts":"2024-12-03T06:22:31.909Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2890}
{"level":"info","ts":"2024-12-03T06:22:31.911Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":2890,"took":"1.37422ms","hash":2771112021}
{"level":"info","ts":"2024-12-03T06:22:31.911Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2771112021,"revision":2890,"compact-revision":2651}
{"level":"info","ts":"2024-12-03T06:27:31.930Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3132}
{"level":"info","ts":"2024-12-03T06:27:31.931Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":3132,"took":"1.062225ms","hash":1588422302}
{"level":"info","ts":"2024-12-03T06:27:31.931Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1588422302,"revision":3132,"compact-revision":2890}
WARNING: 2024/12/03 06:31:54 [core] grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
{"level":"info","ts":"2024-12-03T06:32:31.955Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3412}
{"level":"info","ts":"2024-12-03T06:32:32.004Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":3412,"took":"47.667805ms","hash":3052839868}
{"level":"info","ts":"2024-12-03T06:32:32.004Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3052839868,"revision":3412,"compact-revision":3132}
{"level":"info","ts":"2024-12-03T06:37:31.968Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3654}
{"level":"info","ts":"2024-12-03T06:37:31.980Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":3654,"took":"11.897833ms","hash":3750513109}
{"level":"info","ts":"2024-12-03T06:37:31.980Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3750513109,"revision":3654,"compact-revision":3412}
{"level":"info","ts":"2024-12-03T06:43:48.465Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3894}
{"level":"info","ts":"2024-12-03T06:43:48.480Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":3894,"took":"14.877568ms","hash":3740654679}
{"level":"info","ts":"2024-12-03T06:43:48.480Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3740654679,"revision":3894,"compact-revision":3654}
{"level":"info","ts":"2024-12-03T06:48:48.476Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4132}
{"level":"info","ts":"2024-12-03T06:48:48.478Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":4132,"took":"1.446935ms","hash":3840312035}
{"level":"info","ts":"2024-12-03T06:48:48.478Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3840312035,"revision":4132,"compact-revision":3894}
{"level":"info","ts":"2024-12-03T06:53:48.498Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4374}
{"level":"info","ts":"2024-12-03T06:53:48.511Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":4374,"took":"13.209934ms","hash":1745640983}
{"level":"info","ts":"2024-12-03T06:53:48.512Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1745640983,"revision":4374,"compact-revision":4132}
{"level":"info","ts":"2024-12-03T06:58:48.520Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4628}
{"level":"info","ts":"2024-12-03T06:58:48.542Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":4628,"took":"21.733595ms","hash":4129550516}
{"level":"info","ts":"2024-12-03T06:58:48.543Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4129550516,"revision":4628,"compact-revision":4374}
{"level":"info","ts":"2024-12-03T07:03:48.552Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4870}
{"level":"info","ts":"2024-12-03T07:03:48.557Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":4870,"took":"3.855039ms","hash":753374365}
{"level":"info","ts":"2024-12-03T07:03:48.557Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":753374365,"revision":4870,"compact-revision":4628}
{"level":"info","ts":"2024-12-03T07:08:48.579Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5111}
{"level":"info","ts":"2024-12-03T07:08:48.581Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":5111,"took":"1.044643ms","hash":4001348268}
{"level":"info","ts":"2024-12-03T07:08:48.581Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4001348268,"revision":5111,"compact-revision":4870}
{"level":"info","ts":"2024-12-03T07:13:48.605Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5354}
{"level":"info","ts":"2024-12-03T07:13:48.606Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":5354,"took":"1.11658ms","hash":1591930416}
{"level":"info","ts":"2024-12-03T07:13:48.606Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1591930416,"revision":5354,"compact-revision":5111}
{"level":"info","ts":"2024-12-03T07:18:48.626Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5594}
{"level":"info","ts":"2024-12-03T07:18:48.641Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":5594,"took":"14.183257ms","hash":2412232873}
{"level":"info","ts":"2024-12-03T07:18:48.641Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2412232873,"revision":5594,"compact-revision":5354}
{"level":"info","ts":"2024-12-03T07:21:10.796Z","caller":"traceutil/trace.go:171","msg":"trace[1846152149] transaction","detail":"{read_only:false; response_revision:5946; number_of_response:1; }","duration":"124.587423ms","start":"2024-12-03T07:21:10.672Z","end":"2024-12-03T07:21:10.796Z","steps":["trace[1846152149] 'process raft request'  (duration: 124.282385ms)"],"step_count":1}
{"level":"info","ts":"2024-12-03T07:25:22.400Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5906}
{"level":"info","ts":"2024-12-03T07:25:22.421Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":5906,"took":"20.677968ms","hash":3018383574}
{"level":"info","ts":"2024-12-03T07:25:22.421Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3018383574,"revision":5906,"compact-revision":5594}

* 
* ==> etcd [21b77cc84376] <==
* {"level":"info","ts":"2024-12-03T05:39:39.577Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2024-12-03T05:39:39.578Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"info","ts":"2024-12-03T05:39:39.578Z","caller":"embed/etcd.go:124","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-12-03T05:39:39.578Z","caller":"embed/etcd.go:484","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-12-03T05:39:39.579Z","caller":"embed/etcd.go:132","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2024-12-03T05:39:39.579Z","caller":"embed/etcd.go:306","msg":"starting an etcd server","etcd-version":"3.5.7","git-sha":"215b53cf3","go-version":"go1.17.13","go-os":"linux","go-arch":"amd64","max-cpu-set":12,"max-cpu-available":12,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-12-03T05:39:39.580Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"740.679µs"}
{"level":"info","ts":"2024-12-03T05:39:39.586Z","caller":"etcdserver/server.go:530","msg":"No snapshot found. Recovering WAL from scratch!"}
{"level":"info","ts":"2024-12-03T05:39:39.592Z","caller":"etcdserver/raft.go:529","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":1142}
{"level":"info","ts":"2024-12-03T05:39:39.592Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2024-12-03T05:39:39.592Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 2"}
{"level":"info","ts":"2024-12-03T05:39:39.592Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 2, commit: 1142, applied: 0, lastindex: 1142, lastterm: 2]"}
{"level":"warn","ts":"2024-12-03T05:39:39.596Z","caller":"auth/store.go:1234","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-12-03T05:39:39.600Z","caller":"mvcc/kvstore.go:323","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":604}
{"level":"info","ts":"2024-12-03T05:39:39.602Z","caller":"mvcc/kvstore.go:393","msg":"kvstore restored","current-rev":990}
{"level":"info","ts":"2024-12-03T05:39:39.610Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-12-03T05:39:39.618Z","caller":"etcdserver/corrupt.go:95","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2024-12-03T05:39:39.619Z","caller":"etcdserver/corrupt.go:165","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-12-03T05:39:39.619Z","caller":"etcdserver/server.go:854","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.7","cluster-version":"to_be_decided"}
{"level":"info","ts":"2024-12-03T05:39:39.619Z","caller":"etcdserver/server.go:754","msg":"starting initial election tick advance","election-ticks":10}
{"level":"info","ts":"2024-12-03T05:39:39.619Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-12-03T05:39:39.619Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-12-03T05:39:39.619Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-12-03T05:39:39.619Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2024-12-03T05:39:39.619Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-12-03T05:39:39.619Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2024-12-03T05:39:39.619Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-12-03T05:39:39.620Z","caller":"embed/etcd.go:687","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-12-03T05:39:39.621Z","caller":"embed/etcd.go:586","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-12-03T05:39:39.621Z","caller":"embed/etcd.go:558","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-12-03T05:39:39.621Z","caller":"embed/etcd.go:275","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-12-03T05:39:39.621Z","caller":"embed/etcd.go:762","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-12-03T05:39:41.193Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 2"}
{"level":"info","ts":"2024-12-03T05:39:41.194Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 2"}
{"level":"info","ts":"2024-12-03T05:39:41.194Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2024-12-03T05:39:41.194Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 3"}
{"level":"info","ts":"2024-12-03T05:39:41.194Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 3"}
{"level":"info","ts":"2024-12-03T05:39:41.194Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 3"}
{"level":"info","ts":"2024-12-03T05:39:41.194Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 3"}
{"level":"info","ts":"2024-12-03T05:39:41.207Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-12-03T05:39:41.207Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-12-03T05:39:41.208Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-12-03T05:39:41.208Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-12-03T05:39:41.208Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-12-03T05:39:41.210Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-12-03T05:39:41.211Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-12-03T05:42:22.003Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2024-12-03T05:42:22.003Z","caller":"embed/etcd.go:373","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"info","ts":"2024-12-03T05:42:22.025Z","caller":"etcdserver/server.go:1465","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-12-03T05:42:22.038Z","caller":"embed/etcd.go:568","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-12-03T05:42:22.040Z","caller":"embed/etcd.go:573","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-12-03T05:42:22.040Z","caller":"embed/etcd.go:375","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}

* 
* ==> kernel <==
*  07:28:16 up 19:14,  0 users,  load average: 2.15, 1.98, 1.80
Linux minikube 6.11.0-9-generic #9-Ubuntu SMP PREEMPT_DYNAMIC Mon Oct 14 13:19:59 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 20.04.5 LTS"

* 
* ==> kube-apiserver [00c7c5c840fe] <==
*   "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1203 05:42:22.012635       1 logging.go:59] [core] [Channel #136 SubChannel #137] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1203 05:42:22.012699       1 logging.go:59] [core] [Channel #172 SubChannel #173] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1203 05:42:22.012700       1 logging.go:59] [core] [Channel #121 SubChannel #122] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1203 05:42:22.012739       1 logging.go:59] [core] [Channel #109 SubChannel #110] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1203 05:42:22.012869       1 logging.go:59] [core] [Channel #127 SubChannel #128] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1203 05:42:22.013129       1 logging.go:59] [core] [Channel #40 SubChannel #41] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1203 05:42:22.014360       1 logging.go:59] [core] [Channel #157 SubChannel #158] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"

* 
* ==> kube-apiserver [ec71bfd34b24] <==
* I1203 07:18:11.514330       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W1203 07:18:11.515328       1 handler_proxy.go:106] no RequestInfo found in the context
E1203 07:18:11.515370       1 controller.go:116] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I1203 07:18:11.515376       1 controller.go:129] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I1203 07:18:49.044854       1 handler_discovery.go:325] DiscoveryManager: Failed to download discovery for kube-system/metrics-server:443: 503 error trying to reach service: dial tcp 10.101.6.134:443: connect: connection refused
I1203 07:18:49.044908       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1203 07:18:49.174554       1 handler_discovery.go:325] DiscoveryManager: Failed to download discovery for kube-system/metrics-server:443: 503 error trying to reach service: dial tcp 10.101.6.134:443: connect: connection refused
I1203 07:18:49.174580       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
W1203 07:18:50.174221       1 handler_proxy.go:106] no RequestInfo found in the context
W1203 07:18:50.174267       1 handler_proxy.go:106] no RequestInfo found in the context
E1203 07:18:50.174341       1 controller.go:116] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I1203 07:18:50.174376       1 controller.go:129] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E1203 07:18:50.174369       1 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I1203 07:18:50.176126       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I1203 07:21:22.797537       1 handler_discovery.go:325] DiscoveryManager: Failed to download discovery for kube-system/metrics-server:443: 503 error trying to reach service: dial tcp 10.101.6.134:443: connect: connection refused
I1203 07:21:22.797618       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
W1203 07:21:23.926407       1 handler_proxy.go:106] no RequestInfo found in the context
E1203 07:21:23.926499       1 controller.go:116] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I1203 07:21:23.926518       1 controller.go:129] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W1203 07:21:23.928694       1 handler_proxy.go:106] no RequestInfo found in the context
E1203 07:21:23.928746       1 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I1203 07:21:23.928757       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I1203 07:22:22.797054       1 handler_discovery.go:325] DiscoveryManager: Failed to download discovery for kube-system/metrics-server:443: 503 error trying to reach service: dial tcp 10.101.6.134:443: connect: connection refused
I1203 07:22:22.797103       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1203 07:23:22.796340       1 handler_discovery.go:325] DiscoveryManager: Failed to download discovery for kube-system/metrics-server:443: 503 error trying to reach service: dial tcp 10.101.6.134:443: connect: connection refused
I1203 07:23:22.796404       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
W1203 07:23:23.927727       1 handler_proxy.go:106] no RequestInfo found in the context
E1203 07:23:23.927844       1 controller.go:116] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I1203 07:23:23.927859       1 controller.go:129] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W1203 07:23:23.929820       1 handler_proxy.go:106] no RequestInfo found in the context
E1203 07:23:23.929859       1 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I1203 07:23:23.929872       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I1203 07:24:22.796984       1 handler_discovery.go:325] DiscoveryManager: Failed to download discovery for kube-system/metrics-server:443: 503 error trying to reach service: dial tcp 10.101.6.134:443: connect: connection refused
I1203 07:24:22.797020       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1203 07:25:22.797156       1 handler_discovery.go:325] DiscoveryManager: Failed to download discovery for kube-system/metrics-server:443: 503 error trying to reach service: dial tcp 10.101.6.134:443: connect: connection refused
I1203 07:25:22.797255       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1203 07:25:24.963341       1 handler_discovery.go:325] DiscoveryManager: Failed to download discovery for kube-system/metrics-server:443: 503 error trying to reach service: dial tcp 10.101.6.134:443: connect: connection refused
I1203 07:25:24.963371       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
W1203 07:25:25.963657       1 handler_proxy.go:106] no RequestInfo found in the context
W1203 07:25:25.963692       1 handler_proxy.go:106] no RequestInfo found in the context
E1203 07:25:25.963757       1 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I1203 07:25:25.963787       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E1203 07:25:25.963905       1 controller.go:116] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I1203 07:25:25.965061       1 controller.go:129] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I1203 07:26:22.796521       1 handler_discovery.go:325] DiscoveryManager: Failed to download discovery for kube-system/metrics-server:443: 503 error trying to reach service: dial tcp 10.101.6.134:443: connect: connection refused
I1203 07:26:22.796552       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
W1203 07:26:25.964984       1 handler_proxy.go:106] no RequestInfo found in the context
E1203 07:26:25.965056       1 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I1203 07:26:25.965075       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W1203 07:26:25.965329       1 handler_proxy.go:106] no RequestInfo found in the context
E1203 07:26:25.965537       1 controller.go:116] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I1203 07:26:25.967091       1 controller.go:129] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I1203 07:27:22.797289       1 handler_discovery.go:325] DiscoveryManager: Failed to download discovery for kube-system/metrics-server:443: 503 error trying to reach service: dial tcp 10.101.6.134:443: connect: connection refused
I1203 07:27:22.797334       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager

* 
* ==> kube-controller-manager [897c9a4718d4] <==
* I1203 07:25:25.367769       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)"
E1203 07:25:25.380927       1 horizontal.go:274] failed to compute desired number of replicas based on listed metrics for Deployment/kubernetes-springboot/kubernetes-springboot-deployment: invalid metrics (2 invalid out of 2), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)
I1203 07:25:25.381087       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetPodsMetric" message="unable to get metric requests-per-second: unable to fetch metrics from custom metrics API: no custom metrics API (custom.metrics.k8s.io) registered"
I1203 07:25:25.381147       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (2 invalid out of 2), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)"
E1203 07:25:36.811945       1 resource_quota_controller.go:441] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
W1203 07:25:37.204655       1 garbagecollector.go:816] failed to discover some groups: map[metrics.k8s.io/v1beta1:stale GroupVersion discovery: metrics.k8s.io/v1beta1]
I1203 07:25:40.389329       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)"
E1203 07:25:40.393959       1 horizontal.go:274] failed to compute desired number of replicas based on listed metrics for Deployment/kubernetes-springboot/kubernetes-springboot-deployment: invalid metrics (2 invalid out of 2), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)
I1203 07:25:40.394094       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetPodsMetric" message="unable to get metric requests-per-second: unable to fetch metrics from custom metrics API: no custom metrics API (custom.metrics.k8s.io) registered"
I1203 07:25:40.394153       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (2 invalid out of 2), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)"
I1203 07:25:55.409537       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)"
E1203 07:25:55.414279       1 horizontal.go:274] failed to compute desired number of replicas based on listed metrics for Deployment/kubernetes-springboot/kubernetes-springboot-deployment: invalid metrics (2 invalid out of 2), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)
I1203 07:25:55.414340       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetPodsMetric" message="unable to get metric requests-per-second: unable to fetch metrics from custom metrics API: no custom metrics API (custom.metrics.k8s.io) registered"
I1203 07:25:55.414382       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (2 invalid out of 2), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)"
E1203 07:26:06.816837       1 resource_quota_controller.go:441] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
W1203 07:26:07.217909       1 garbagecollector.go:816] failed to discover some groups: map[metrics.k8s.io/v1beta1:stale GroupVersion discovery: metrics.k8s.io/v1beta1]
I1203 07:26:10.419188       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)"
E1203 07:26:10.422746       1 horizontal.go:274] failed to compute desired number of replicas based on listed metrics for Deployment/kubernetes-springboot/kubernetes-springboot-deployment: invalid metrics (2 invalid out of 2), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)
I1203 07:26:10.422814       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetPodsMetric" message="unable to get metric requests-per-second: unable to fetch metrics from custom metrics API: no custom metrics API (custom.metrics.k8s.io) registered"
I1203 07:26:10.422842       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (2 invalid out of 2), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)"
I1203 07:26:25.439681       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)"
E1203 07:26:25.443438       1 horizontal.go:274] failed to compute desired number of replicas based on listed metrics for Deployment/kubernetes-springboot/kubernetes-springboot-deployment: invalid metrics (2 invalid out of 2), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)
I1203 07:26:25.443513       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetPodsMetric" message="unable to get metric requests-per-second: unable to fetch metrics from custom metrics API: no custom metrics API (custom.metrics.k8s.io) registered"
I1203 07:26:25.443557       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (2 invalid out of 2), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)"
E1203 07:26:36.844401       1 resource_quota_controller.go:441] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
W1203 07:26:37.241209       1 garbagecollector.go:816] failed to discover some groups: map[metrics.k8s.io/v1beta1:stale GroupVersion discovery: metrics.k8s.io/v1beta1]
I1203 07:26:40.449012       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)"
E1203 07:26:40.452930       1 horizontal.go:274] failed to compute desired number of replicas based on listed metrics for Deployment/kubernetes-springboot/kubernetes-springboot-deployment: invalid metrics (2 invalid out of 2), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)
I1203 07:26:40.452986       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetPodsMetric" message="unable to get metric requests-per-second: unable to fetch metrics from custom metrics API: no custom metrics API (custom.metrics.k8s.io) registered"
I1203 07:26:40.453028       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (2 invalid out of 2), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)"
I1203 07:26:55.484920       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)"
E1203 07:26:55.489494       1 horizontal.go:274] failed to compute desired number of replicas based on listed metrics for Deployment/kubernetes-springboot/kubernetes-springboot-deployment: invalid metrics (2 invalid out of 2), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)
I1203 07:26:55.489547       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetPodsMetric" message="unable to get metric requests-per-second: unable to fetch metrics from custom metrics API: no custom metrics API (custom.metrics.k8s.io) registered"
I1203 07:26:55.489599       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (2 invalid out of 2), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)"
E1203 07:27:06.851503       1 resource_quota_controller.go:441] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
W1203 07:27:07.259806       1 garbagecollector.go:816] failed to discover some groups: map[metrics.k8s.io/v1beta1:stale GroupVersion discovery: metrics.k8s.io/v1beta1]
I1203 07:27:10.494987       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)"
E1203 07:27:10.498094       1 horizontal.go:274] failed to compute desired number of replicas based on listed metrics for Deployment/kubernetes-springboot/kubernetes-springboot-deployment: invalid metrics (2 invalid out of 2), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)
I1203 07:27:10.498215       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetPodsMetric" message="unable to get metric requests-per-second: unable to fetch metrics from custom metrics API: no custom metrics API (custom.metrics.k8s.io) registered"
I1203 07:27:10.498251       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (2 invalid out of 2), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)"
I1203 07:27:25.515162       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)"
E1203 07:27:25.519094       1 horizontal.go:274] failed to compute desired number of replicas based on listed metrics for Deployment/kubernetes-springboot/kubernetes-springboot-deployment: invalid metrics (2 invalid out of 2), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)
I1203 07:27:25.519134       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetPodsMetric" message="unable to get metric requests-per-second: unable to fetch metrics from custom metrics API: no custom metrics API (custom.metrics.k8s.io) registered"
I1203 07:27:25.519185       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (2 invalid out of 2), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)"
E1203 07:27:36.858435       1 resource_quota_controller.go:441] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
W1203 07:27:37.273303       1 garbagecollector.go:816] failed to discover some groups: map[metrics.k8s.io/v1beta1:stale GroupVersion discovery: metrics.k8s.io/v1beta1]
I1203 07:27:40.527302       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)"
E1203 07:27:40.534903       1 horizontal.go:274] failed to compute desired number of replicas based on listed metrics for Deployment/kubernetes-springboot/kubernetes-springboot-deployment: invalid metrics (2 invalid out of 2), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)
I1203 07:27:40.534980       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetPodsMetric" message="unable to get metric requests-per-second: unable to fetch metrics from custom metrics API: no custom metrics API (custom.metrics.k8s.io) registered"
I1203 07:27:40.535039       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (2 invalid out of 2), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)"
I1203 07:27:55.554467       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)"
E1203 07:27:55.558831       1 horizontal.go:274] failed to compute desired number of replicas based on listed metrics for Deployment/kubernetes-springboot/kubernetes-springboot-deployment: invalid metrics (2 invalid out of 2), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)
I1203 07:27:55.558902       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetPodsMetric" message="unable to get metric requests-per-second: unable to fetch metrics from custom metrics API: no custom metrics API (custom.metrics.k8s.io) registered"
I1203 07:27:55.558954       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (2 invalid out of 2), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)"
E1203 07:28:06.865115       1 resource_quota_controller.go:441] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1
W1203 07:28:07.282905       1 garbagecollector.go:816] failed to discover some groups: map[metrics.k8s.io/v1beta1:stale GroupVersion discovery: metrics.k8s.io/v1beta1]
I1203 07:28:10.566218       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)"
E1203 07:28:10.569169       1 horizontal.go:274] failed to compute desired number of replicas based on listed metrics for Deployment/kubernetes-springboot/kubernetes-springboot-deployment: invalid metrics (2 invalid out of 2), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)
I1203 07:28:10.569246       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetPodsMetric" message="unable to get metric requests-per-second: unable to fetch metrics from custom metrics API: no custom metrics API (custom.metrics.k8s.io) registered"
I1203 07:28:10.569291       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (2 invalid out of 2), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)"

* 
* ==> kube-controller-manager [95012be6d398] <==
* I1203 05:39:54.784836       1 shared_informer.go:318] Caches are synced for PVC protection
I1203 05:39:54.785965       1 shared_informer.go:318] Caches are synced for service account
I1203 05:39:54.788092       1 shared_informer.go:318] Caches are synced for ReplicaSet
I1203 05:39:54.790569       1 shared_informer.go:318] Caches are synced for disruption
I1203 05:39:54.792699       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I1203 05:39:54.792743       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I1203 05:39:54.793950       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I1203 05:39:54.793982       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I1203 05:39:54.795161       1 shared_informer.go:318] Caches are synced for certificate-csrapproving
I1203 05:39:54.795177       1 shared_informer.go:318] Caches are synced for endpoint_slice
I1203 05:39:54.797637       1 shared_informer.go:318] Caches are synced for deployment
I1203 05:39:54.800106       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I1203 05:39:54.800108       1 shared_informer.go:318] Caches are synced for job
I1203 05:39:54.817770       1 shared_informer.go:318] Caches are synced for node
I1203 05:39:54.817838       1 range_allocator.go:174] "Sending events to api server"
I1203 05:39:54.817873       1 range_allocator.go:178] "Starting range CIDR allocator"
I1203 05:39:54.817882       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I1203 05:39:54.817888       1 shared_informer.go:318] Caches are synced for cidrallocator
I1203 05:39:54.824521       1 shared_informer.go:318] Caches are synced for TTL after finished
I1203 05:39:54.826894       1 shared_informer.go:318] Caches are synced for crt configmap
I1203 05:39:54.832658       1 shared_informer.go:318] Caches are synced for endpoint
I1203 05:39:54.835615       1 shared_informer.go:318] Caches are synced for ephemeral
I1203 05:39:54.839748       1 shared_informer.go:318] Caches are synced for resource quota
I1203 05:39:54.844789       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I1203 05:39:54.903175       1 shared_informer.go:318] Caches are synced for resource quota
I1203 05:39:54.968699       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I1203 05:39:54.969333       1 shared_informer.go:318] Caches are synced for persistent volume
I1203 05:39:54.983358       1 shared_informer.go:318] Caches are synced for expand
I1203 05:39:54.998561       1 shared_informer.go:318] Caches are synced for attach detach
I1203 05:39:55.034515       1 shared_informer.go:318] Caches are synced for PV protection
I1203 05:39:55.350786       1 shared_informer.go:318] Caches are synced for garbage collector
I1203 05:39:55.374581       1 shared_informer.go:318] Caches are synced for garbage collector
I1203 05:39:55.374635       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
E1203 05:40:09.769290       1 horizontal.go:274] failed to compute desired number of replicas based on listed metrics for Deployment/kubernetes-springboot/kubernetes-springboot-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
I1203 05:40:09.769342       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)"
I1203 05:40:09.769373       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)"
E1203 05:40:24.759997       1 horizontal.go:274] failed to compute desired number of replicas based on listed metrics for Deployment/kubernetes-springboot/kubernetes-springboot-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
I1203 05:40:24.760096       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)"
I1203 05:40:24.760132       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)"
E1203 05:40:39.766914       1 horizontal.go:274] failed to compute desired number of replicas based on listed metrics for Deployment/kubernetes-springboot/kubernetes-springboot-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
I1203 05:40:39.766974       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)"
I1203 05:40:39.767032       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)"
E1203 05:40:54.783086       1 horizontal.go:274] failed to compute desired number of replicas based on listed metrics for Deployment/kubernetes-springboot/kubernetes-springboot-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
I1203 05:40:54.783153       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)"
I1203 05:40:54.783209       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)"
E1203 05:41:09.789117       1 horizontal.go:274] failed to compute desired number of replicas based on listed metrics for Deployment/kubernetes-springboot/kubernetes-springboot-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
I1203 05:41:09.789177       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)"
I1203 05:41:09.789215       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)"
E1203 05:41:24.804291       1 horizontal.go:274] failed to compute desired number of replicas based on listed metrics for Deployment/kubernetes-springboot/kubernetes-springboot-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
I1203 05:41:24.804360       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)"
I1203 05:41:24.804399       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)"
E1203 05:41:39.812763       1 horizontal.go:274] failed to compute desired number of replicas based on listed metrics for Deployment/kubernetes-springboot/kubernetes-springboot-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
I1203 05:41:39.812835       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)"
I1203 05:41:39.812861       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)"
E1203 05:41:54.824991       1 horizontal.go:274] failed to compute desired number of replicas based on listed metrics for Deployment/kubernetes-springboot/kubernetes-springboot-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
I1203 05:41:54.825065       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)"
I1203 05:41:54.825112       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)"
E1203 05:42:09.831928       1 horizontal.go:274] failed to compute desired number of replicas based on listed metrics for Deployment/kubernetes-springboot/kubernetes-springboot-deployment: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
I1203 05:42:09.831940       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)"
I1203 05:42:09.831986       1 event.go:307] "Event occurred" object="kubernetes-springboot/kubernetes-springboot-hpa" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)"

* 
* ==> kube-proxy [328d43db0076] <==
* I1203 05:45:52.999635       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I1203 05:45:52.999698       1 server_others.go:110] "Detected node IP" address="192.168.49.2"
I1203 05:45:52.999728       1 server_others.go:551] "Using iptables proxy"
I1203 05:45:53.014980       1 server_others.go:190] "Using iptables Proxier"
I1203 05:45:53.014997       1 server_others.go:197] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I1203 05:45:53.015001       1 server_others.go:198] "Creating dualStackProxier for iptables"
I1203 05:45:53.015008       1 server_others.go:481] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, defaulting to no-op detect-local for IPv6"
I1203 05:45:53.015029       1 proxier.go:253] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I1203 05:45:53.015505       1 server.go:657] "Version info" version="v1.27.0-rc.0"
I1203 05:45:53.015513       1 server.go:659] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1203 05:45:53.016102       1 config.go:315] "Starting node config controller"
I1203 05:45:53.016111       1 shared_informer.go:311] Waiting for caches to sync for node config
I1203 05:45:53.016107       1 config.go:188] "Starting service config controller"
I1203 05:45:53.016118       1 shared_informer.go:311] Waiting for caches to sync for service config
I1203 05:45:53.016131       1 config.go:97] "Starting endpoint slice config controller"
I1203 05:45:53.016192       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I1203 05:45:53.116901       1 shared_informer.go:318] Caches are synced for node config
I1203 05:45:53.117225       1 shared_informer.go:318] Caches are synced for endpoint slice config
I1203 05:45:53.117332       1 shared_informer.go:318] Caches are synced for service config

* 
* ==> kube-proxy [f1dfabf5b507] <==
* I1203 05:39:43.301898       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I1203 05:39:43.301953       1 server_others.go:110] "Detected node IP" address="192.168.49.2"
I1203 05:39:43.301973       1 server_others.go:551] "Using iptables proxy"
I1203 05:39:43.314361       1 server_others.go:190] "Using iptables Proxier"
I1203 05:39:43.314383       1 server_others.go:197] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I1203 05:39:43.314390       1 server_others.go:198] "Creating dualStackProxier for iptables"
I1203 05:39:43.314398       1 server_others.go:481] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, defaulting to no-op detect-local for IPv6"
I1203 05:39:43.314423       1 proxier.go:253] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I1203 05:39:43.314943       1 server.go:657] "Version info" version="v1.27.0-rc.0"
I1203 05:39:43.314953       1 server.go:659] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1203 05:39:43.315487       1 config.go:188] "Starting service config controller"
I1203 05:39:43.315513       1 shared_informer.go:311] Waiting for caches to sync for service config
I1203 05:39:43.315526       1 config.go:97] "Starting endpoint slice config controller"
I1203 05:39:43.315536       1 config.go:315] "Starting node config controller"
I1203 05:39:43.315542       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I1203 05:39:43.315553       1 shared_informer.go:311] Waiting for caches to sync for node config
I1203 05:39:43.415920       1 shared_informer.go:318] Caches are synced for endpoint slice config
I1203 05:39:43.415925       1 shared_informer.go:318] Caches are synced for node config
I1203 05:39:43.415971       1 shared_informer.go:318] Caches are synced for service config

* 
* ==> kube-scheduler [547a9b5e3360] <==
* I1203 05:39:40.101049       1 serving.go:348] Generated self-signed cert in-memory
W1203 05:39:42.036274       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1203 05:39:42.036305       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system": RBAC: [clusterrole.rbac.authorization.k8s.io "system:public-info-viewer" not found, clusterrole.rbac.authorization.k8s.io "system:kube-scheduler" not found, clusterrole.rbac.authorization.k8s.io "system:basic-user" not found, clusterrole.rbac.authorization.k8s.io "system:discovery" not found, clusterrole.rbac.authorization.k8s.io "system:volume-scheduler" not found]
W1203 05:39:42.036315       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W1203 05:39:42.036322       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1203 05:39:42.043288       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.27.0-rc.0"
I1203 05:39:42.043303       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1203 05:39:42.043897       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1203 05:39:42.043912       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1203 05:39:42.044196       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I1203 05:39:42.044222       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1203 05:39:42.144589       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1203 05:42:22.004438       1 secure_serving.go:255] Stopped listening on 127.0.0.1:10259
I1203 05:42:22.004510       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
E1203 05:42:22.004587       1 scheduling_queue.go:1137] "Error while retrieving next pod from scheduling queue" err="scheduling queue is closed"
E1203 05:42:22.004630       1 run.go:74] "command failed" err="finished without leader elect"

* 
* ==> kube-scheduler [79a9d4f6b367] <==
* I1203 05:45:49.379363       1 serving.go:348] Generated self-signed cert in-memory
W1203 05:45:51.333119       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1203 05:45:51.333217       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1203 05:45:51.333241       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W1203 05:45:51.333251       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1203 05:45:51.351391       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.27.0-rc.0"
I1203 05:45:51.351409       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1203 05:45:51.352212       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1203 05:45:51.352223       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1203 05:45:51.352549       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I1203 05:45:51.352576       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1203 05:45:51.452602       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kubelet <==
* -- Logs begin at Tue 2024-12-03 05:45:31 UTC, end at Tue 2024-12-03 07:28:16 UTC. --
Dec 03 07:15:55 minikube kubelet[1621]: E1203 07:15:55.055063    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: Get \\\"https://k8s.gcr.io/v2/\\\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:16:08 minikube kubelet[1621]: E1203 07:16:08.030648    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/metrics-server/metrics-server:v0.5.0\\\"\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:16:36 minikube kubelet[1621]: E1203 07:16:36.078162    1621 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)" image="k8s.gcr.io/metrics-server/metrics-server:v0.5.0"
Dec 03 07:16:36 minikube kubelet[1621]: E1203 07:16:36.078279    1621 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: Get \"https://k8s.gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)" image="k8s.gcr.io/metrics-server/metrics-server:v0.5.0"
Dec 03 07:16:36 minikube kubelet[1621]: E1203 07:16:36.078577    1621 kuberuntime_manager.go:1212] container &Container{Name:metrics-server,Image:k8s.gcr.io/metrics-server/metrics-server:v0.5.0,Command:[],Args:[--cert-dir=/tmp --secure-port=443 --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --kubelet-use-node-status-port --metric-resolution=15s --kubelet-insecure-tls],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:https,HostPort:0,ContainerPort:443,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{209715200 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:tmp-dir,ReadOnly:false,MountPath:/tmp,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-fsfvh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/livez,Port:{1 0 https},Host:,Scheme:HTTPS,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,},ReadinessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readyz,Port:{1 0 https},Host:,Scheme:HTTPS,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:20,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,},Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:*1000,RunAsNonRoot:*true,ReadOnlyRootFilesystem:*true,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod metrics-server-76f7df5ddd-2mwgj_kube-system(2d0765fd-9350-4375-837f-6c82011b839e): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: Get "https://k8s.gcr.io/v2/": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
Dec 03 07:16:36 minikube kubelet[1621]: E1203 07:16:36.078678    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: Get \\\"https://k8s.gcr.io/v2/\\\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:16:48 minikube kubelet[1621]: E1203 07:16:48.039313    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/metrics-server/metrics-server:v0.5.0\\\"\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:17:00 minikube kubelet[1621]: E1203 07:17:00.028430    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/metrics-server/metrics-server:v0.5.0\\\"\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:17:12 minikube kubelet[1621]: E1203 07:17:12.027706    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/metrics-server/metrics-server:v0.5.0\\\"\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:17:38 minikube kubelet[1621]: E1203 07:17:38.064304    1621 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Get \"https://k8s.gcr.io/v2/\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)" image="k8s.gcr.io/metrics-server/metrics-server:v0.5.0"
Dec 03 07:17:38 minikube kubelet[1621]: E1203 07:17:38.064390    1621 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: Get \"https://k8s.gcr.io/v2/\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)" image="k8s.gcr.io/metrics-server/metrics-server:v0.5.0"
Dec 03 07:17:38 minikube kubelet[1621]: E1203 07:17:38.064659    1621 kuberuntime_manager.go:1212] container &Container{Name:metrics-server,Image:k8s.gcr.io/metrics-server/metrics-server:v0.5.0,Command:[],Args:[--cert-dir=/tmp --secure-port=443 --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --kubelet-use-node-status-port --metric-resolution=15s --kubelet-insecure-tls],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:https,HostPort:0,ContainerPort:443,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{209715200 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:tmp-dir,ReadOnly:false,MountPath:/tmp,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-fsfvh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/livez,Port:{1 0 https},Host:,Scheme:HTTPS,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,},ReadinessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readyz,Port:{1 0 https},Host:,Scheme:HTTPS,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:20,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,},Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:*1000,RunAsNonRoot:*true,ReadOnlyRootFilesystem:*true,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod metrics-server-76f7df5ddd-2mwgj_kube-system(2d0765fd-9350-4375-837f-6c82011b839e): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: Get "https://k8s.gcr.io/v2/": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
Dec 03 07:17:38 minikube kubelet[1621]: E1203 07:17:38.064731    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: Get \\\"https://k8s.gcr.io/v2/\\\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:17:52 minikube kubelet[1621]: E1203 07:17:52.029672    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/metrics-server/metrics-server:v0.5.0\\\"\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:18:03 minikube kubelet[1621]: E1203 07:18:03.028555    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/metrics-server/metrics-server:v0.5.0\\\"\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:18:14 minikube kubelet[1621]: E1203 07:18:14.029163    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/metrics-server/metrics-server:v0.5.0\\\"\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:18:26 minikube kubelet[1621]: E1203 07:18:26.028977    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/metrics-server/metrics-server:v0.5.0\\\"\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:18:38 minikube kubelet[1621]: E1203 07:18:38.028444    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/metrics-server/metrics-server:v0.5.0\\\"\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:18:49 minikube kubelet[1621]: E1203 07:18:49.030395    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/metrics-server/metrics-server:v0.5.0\\\"\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:19:11 minikube kubelet[1621]: E1203 07:19:11.398141    1621 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Get \"https://k8s.gcr.io/v2/\": dial tcp 66.102.1.82:443: connect: no route to host" image="k8s.gcr.io/metrics-server/metrics-server:v0.5.0"
Dec 03 07:19:11 minikube kubelet[1621]: E1203 07:19:11.398360    1621 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: Get \"https://k8s.gcr.io/v2/\": dial tcp 66.102.1.82:443: connect: no route to host" image="k8s.gcr.io/metrics-server/metrics-server:v0.5.0"
Dec 03 07:19:11 minikube kubelet[1621]: E1203 07:19:11.399155    1621 kuberuntime_manager.go:1212] container &Container{Name:metrics-server,Image:k8s.gcr.io/metrics-server/metrics-server:v0.5.0,Command:[],Args:[--cert-dir=/tmp --secure-port=443 --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --kubelet-use-node-status-port --metric-resolution=15s --kubelet-insecure-tls],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:https,HostPort:0,ContainerPort:443,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{209715200 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:tmp-dir,ReadOnly:false,MountPath:/tmp,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-fsfvh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/livez,Port:{1 0 https},Host:,Scheme:HTTPS,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,},ReadinessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readyz,Port:{1 0 https},Host:,Scheme:HTTPS,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:20,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,},Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:*1000,RunAsNonRoot:*true,ReadOnlyRootFilesystem:*true,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod metrics-server-76f7df5ddd-2mwgj_kube-system(2d0765fd-9350-4375-837f-6c82011b839e): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: Get "https://k8s.gcr.io/v2/": dial tcp 66.102.1.82:443: connect: no route to host
Dec 03 07:19:11 minikube kubelet[1621]: E1203 07:19:11.399389    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: Get \\\"https://k8s.gcr.io/v2/\\\": dial tcp 66.102.1.82:443: connect: no route to host\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:20:59 minikube kubelet[1621]: E1203 07:20:59.783211    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/metrics-server/metrics-server:v0.5.0\\\"\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:21:14 minikube kubelet[1621]: E1203 07:21:14.782651    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/metrics-server/metrics-server:v0.5.0\\\"\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:21:26 minikube kubelet[1621]: E1203 07:21:26.780835    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/metrics-server/metrics-server:v0.5.0\\\"\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:21:38 minikube kubelet[1621]: E1203 07:21:38.780895    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/metrics-server/metrics-server:v0.5.0\\\"\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:21:52 minikube kubelet[1621]: E1203 07:21:52.781944    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/metrics-server/metrics-server:v0.5.0\\\"\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:22:04 minikube kubelet[1621]: E1203 07:22:04.782557    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/metrics-server/metrics-server:v0.5.0\\\"\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:22:15 minikube kubelet[1621]: E1203 07:22:15.782015    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/metrics-server/metrics-server:v0.5.0\\\"\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:22:28 minikube kubelet[1621]: E1203 07:22:28.782179    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/metrics-server/metrics-server:v0.5.0\\\"\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:22:42 minikube kubelet[1621]: E1203 07:22:42.780413    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/metrics-server/metrics-server:v0.5.0\\\"\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:22:54 minikube kubelet[1621]: E1203 07:22:54.782544    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/metrics-server/metrics-server:v0.5.0\\\"\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:23:06 minikube kubelet[1621]: E1203 07:23:06.781631    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/metrics-server/metrics-server:v0.5.0\\\"\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:23:19 minikube kubelet[1621]: E1203 07:23:19.781477    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/metrics-server/metrics-server:v0.5.0\\\"\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:23:42 minikube kubelet[1621]: E1203 07:23:42.673747    1621 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Get \"https://registry.k8s.io/v2/metrics-server/metrics-server/manifests/sha256:6c5603956c0aed6b4087a8716afce8eb22f664b13162346ee852b4fab305ca15\": dial tcp [2600:1901:0:bbc4::]:443: connect: network is unreachable" image="k8s.gcr.io/metrics-server/metrics-server:v0.5.0"
Dec 03 07:23:42 minikube kubelet[1621]: E1203 07:23:42.673983    1621 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: Get \"https://registry.k8s.io/v2/metrics-server/metrics-server/manifests/sha256:6c5603956c0aed6b4087a8716afce8eb22f664b13162346ee852b4fab305ca15\": dial tcp [2600:1901:0:bbc4::]:443: connect: network is unreachable" image="k8s.gcr.io/metrics-server/metrics-server:v0.5.0"
Dec 03 07:23:42 minikube kubelet[1621]: E1203 07:23:42.674272    1621 kuberuntime_manager.go:1212] container &Container{Name:metrics-server,Image:k8s.gcr.io/metrics-server/metrics-server:v0.5.0,Command:[],Args:[--cert-dir=/tmp --secure-port=443 --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --kubelet-use-node-status-port --metric-resolution=15s --kubelet-insecure-tls],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:https,HostPort:0,ContainerPort:443,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{209715200 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:tmp-dir,ReadOnly:false,MountPath:/tmp,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-fsfvh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/livez,Port:{1 0 https},Host:,Scheme:HTTPS,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,},ReadinessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readyz,Port:{1 0 https},Host:,Scheme:HTTPS,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:20,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,},Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:*1000,RunAsNonRoot:*true,ReadOnlyRootFilesystem:*true,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod metrics-server-76f7df5ddd-2mwgj_kube-system(2d0765fd-9350-4375-837f-6c82011b839e): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: Get "https://registry.k8s.io/v2/metrics-server/metrics-server/manifests/sha256:6c5603956c0aed6b4087a8716afce8eb22f664b13162346ee852b4fab305ca15": dial tcp [2600:1901:0:bbc4::]:443: connect: network is unreachable
Dec 03 07:23:42 minikube kubelet[1621]: E1203 07:23:42.674371    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: Get \\\"https://registry.k8s.io/v2/metrics-server/metrics-server/manifests/sha256:6c5603956c0aed6b4087a8716afce8eb22f664b13162346ee852b4fab305ca15\\\": dial tcp [2600:1901:0:bbc4::]:443: connect: network is unreachable\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:23:53 minikube kubelet[1621]: E1203 07:23:53.782316    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/metrics-server/metrics-server:v0.5.0\\\"\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:24:08 minikube kubelet[1621]: E1203 07:24:08.781523    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/metrics-server/metrics-server:v0.5.0\\\"\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:24:22 minikube kubelet[1621]: E1203 07:24:22.781472    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/metrics-server/metrics-server:v0.5.0\\\"\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:24:36 minikube kubelet[1621]: E1203 07:24:36.781703    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/metrics-server/metrics-server:v0.5.0\\\"\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:24:49 minikube kubelet[1621]: E1203 07:24:49.782783    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/metrics-server/metrics-server:v0.5.0\\\"\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:25:01 minikube kubelet[1621]: E1203 07:25:01.784620    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/metrics-server/metrics-server:v0.5.0\\\"\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:25:15 minikube kubelet[1621]: E1203 07:25:15.781603    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/metrics-server/metrics-server:v0.5.0\\\"\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:25:30 minikube kubelet[1621]: E1203 07:25:30.781867    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/metrics-server/metrics-server:v0.5.0\\\"\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:25:42 minikube kubelet[1621]: E1203 07:25:42.781920    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/metrics-server/metrics-server:v0.5.0\\\"\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:25:56 minikube kubelet[1621]: E1203 07:25:56.780375    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/metrics-server/metrics-server:v0.5.0\\\"\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:26:07 minikube kubelet[1621]: E1203 07:26:07.782104    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/metrics-server/metrics-server:v0.5.0\\\"\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:26:18 minikube kubelet[1621]: E1203 07:26:18.783037    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/metrics-server/metrics-server:v0.5.0\\\"\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:26:32 minikube kubelet[1621]: E1203 07:26:32.781523    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/metrics-server/metrics-server:v0.5.0\\\"\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:26:43 minikube kubelet[1621]: E1203 07:26:43.781778    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/metrics-server/metrics-server:v0.5.0\\\"\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:26:55 minikube kubelet[1621]: E1203 07:26:55.781841    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/metrics-server/metrics-server:v0.5.0\\\"\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:27:07 minikube kubelet[1621]: E1203 07:27:07.782887    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/metrics-server/metrics-server:v0.5.0\\\"\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:27:20 minikube kubelet[1621]: E1203 07:27:20.781065    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/metrics-server/metrics-server:v0.5.0\\\"\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:27:34 minikube kubelet[1621]: E1203 07:27:34.781177    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/metrics-server/metrics-server:v0.5.0\\\"\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:27:48 minikube kubelet[1621]: E1203 07:27:48.780270    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/metrics-server/metrics-server:v0.5.0\\\"\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:27:59 minikube kubelet[1621]: E1203 07:27:59.782022    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/metrics-server/metrics-server:v0.5.0\\\"\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e
Dec 03 07:28:14 minikube kubelet[1621]: E1203 07:28:14.780373    1621 pod_workers.go:1281] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/metrics-server/metrics-server:v0.5.0\\\"\"" pod="kube-system/metrics-server-76f7df5ddd-2mwgj" podUID=2d0765fd-9350-4375-837f-6c82011b839e

* 
* ==> kubernetes-dashboard [1b9529e755ea] <==
* 2024/12/03 06:36:49 [2024-12-03T06:36:49Z] Outcoming response to 127.0.0.1 with 200 status code
2024/12/03 06:36:49 received 0 resources from sidecar instead of 1
2024/12/03 06:36:49 [2024-12-03T06:36:49Z] Incoming HTTP/1.1 GET /api/v1/replicaset/kubernetes-springboot?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/12/03 06:36:49 Getting list of all replica sets in the cluster
2024/12/03 06:36:49 received 0 resources from sidecar instead of 1
2024/12/03 06:36:49 received 0 resources from sidecar instead of 1
2024/12/03 06:36:49 [2024-12-03T06:36:49Z] Outcoming response to 127.0.0.1 with 200 status code
2024/12/03 06:36:49 received 0 resources from sidecar instead of 1
2024/12/03 06:36:49 Getting pod metrics
2024/12/03 06:36:49 [2024-12-03T06:36:49Z] Incoming HTTP/1.1 GET /api/v1/replicationcontroller/kubernetes-springboot?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/12/03 06:36:49 Getting list of all replication controllers in the cluster
2024/12/03 06:36:49 [2024-12-03T06:36:49Z] Outcoming response to 127.0.0.1 with 200 status code
2024/12/03 06:36:49 received 0 resources from sidecar instead of 1
2024/12/03 06:36:49 received 0 resources from sidecar instead of 1
2024/12/03 06:36:49 [2024-12-03T06:36:49Z] Incoming HTTP/1.1 GET /api/v1/statefulset/kubernetes-springboot?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/12/03 06:36:49 Getting list of all pet sets in the cluster
2024/12/03 06:36:49 received 0 resources from sidecar instead of 1
2024/12/03 06:36:49 received 0 resources from sidecar instead of 1
2024/12/03 06:36:49 Skipping metric because of error: Metric label not set.
2024/12/03 06:36:49 Skipping metric because of error: Metric label not set.
2024/12/03 06:36:49 [2024-12-03T06:36:49Z] Outcoming response to 127.0.0.1 with 200 status code
2024/12/03 06:36:49 [2024-12-03T06:36:49Z] Outcoming response to 127.0.0.1 with 200 status code
2024/12/03 06:36:49 [2024-12-03T06:36:49Z] Outcoming response to 127.0.0.1 with 200 status code
2024/12/03 06:36:52 [2024-12-03T06:36:52Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2024/12/03 06:36:52 Getting list of namespaces
2024/12/03 06:36:52 [2024-12-03T06:36:52Z] Outcoming response to 127.0.0.1 with 200 status code
2024/12/03 06:36:52 [2024-12-03T06:36:52Z] Incoming HTTP/1.1 GET /api/v1/cronjob/kubernetes-springboot?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/12/03 06:36:52 Getting list of all cron jobs in the cluster
2024/12/03 06:36:52 [2024-12-03T06:36:52Z] Outcoming response to 127.0.0.1 with 200 status code
2024/12/03 06:36:53 [2024-12-03T06:36:53Z] Incoming HTTP/1.1 GET /api/v1/daemonset/kubernetes-springboot?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/12/03 06:36:53 [2024-12-03T06:36:53Z] Incoming HTTP/1.1 GET /api/v1/deployment/kubernetes-springboot?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/12/03 06:36:53 Getting list of all deployments in the cluster
2024/12/03 06:36:53 [2024-12-03T06:36:53Z] Outcoming response to 127.0.0.1 with 200 status code
2024/12/03 06:36:53 received 0 resources from sidecar instead of 1
2024/12/03 06:36:53 received 0 resources from sidecar instead of 1
2024/12/03 06:36:53 [2024-12-03T06:36:53Z] Outcoming response to 127.0.0.1 with 200 status code
2024/12/03 06:36:54 [2024-12-03T06:36:54Z] Incoming HTTP/1.1 GET /api/v1/job/kubernetes-springboot?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/12/03 06:36:54 Getting list of all jobs in the cluster
2024/12/03 06:36:54 [2024-12-03T06:36:54Z] Outcoming response to 127.0.0.1 with 200 status code
2024/12/03 06:36:55 [2024-12-03T06:36:55Z] Incoming HTTP/1.1 GET /api/v1/pod/kubernetes-springboot?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/12/03 06:36:55 Getting list of all pods in the cluster
2024/12/03 06:36:55 [2024-12-03T06:36:55Z] Incoming HTTP/1.1 GET /api/v1/replicaset/kubernetes-springboot?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/12/03 06:36:55 Getting list of all replica sets in the cluster
2024/12/03 06:36:55 received 0 resources from sidecar instead of 1
2024/12/03 06:36:55 received 0 resources from sidecar instead of 1
2024/12/03 06:36:55 received 0 resources from sidecar instead of 1
2024/12/03 06:36:55 Getting pod metrics
2024/12/03 06:36:55 received 0 resources from sidecar instead of 1
2024/12/03 06:36:55 [2024-12-03T06:36:55Z] Outcoming response to 127.0.0.1 with 200 status code
2024/12/03 06:36:55 received 0 resources from sidecar instead of 1
2024/12/03 06:36:55 received 0 resources from sidecar instead of 1
2024/12/03 06:36:55 Skipping metric because of error: Metric label not set.
2024/12/03 06:36:55 Skipping metric because of error: Metric label not set.
2024/12/03 06:36:55 [2024-12-03T06:36:55Z] Outcoming response to 127.0.0.1 with 200 status code
2024/12/03 06:36:56 [2024-12-03T06:36:56Z] Incoming HTTP/1.1 GET /api/v1/replicationcontroller/kubernetes-springboot?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/12/03 06:36:56 Getting list of all replication controllers in the cluster
2024/12/03 06:36:56 [2024-12-03T06:36:56Z] Incoming HTTP/1.1 GET /api/v1/statefulset/kubernetes-springboot?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/12/03 06:36:56 Getting list of all pet sets in the cluster
2024/12/03 06:36:56 [2024-12-03T06:36:56Z] Outcoming response to 127.0.0.1 with 200 status code
2024/12/03 06:36:56 [2024-12-03T06:36:56Z] Outcoming response to 127.0.0.1 with 200 status code

* 
* ==> storage-provisioner [c28cc1937611] <==
* I1203 05:46:37.511675       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1203 05:46:37.518296       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1203 05:46:37.518323       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1203 05:46:54.929384       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1203 05:46:54.929534       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"708460fa-74b1-4b85-893d-7bb2688b4999", APIVersion:"v1", ResourceVersion:"1359", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_a4585969-5191-4101-9c40-1aacb3ef6ac9 became leader
I1203 05:46:54.929600       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_a4585969-5191-4101-9c40-1aacb3ef6ac9!
I1203 05:46:55.030408       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_a4585969-5191-4101-9c40-1aacb3ef6ac9!

* 
* ==> storage-provisioner [c599b3ecfb38] <==
* I1203 05:45:52.963486       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1203 05:46:22.965219       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

